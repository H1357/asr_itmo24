{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0eabe",
   "metadata": {},
   "source": [
    "# Языковые модели\n",
    "\n",
    "Языковые модели играют важную роль в системах распознавания речи, помогая создавать более грамотные и лексически корректные тексты. В данной работе мы будем изучать нграмные языковые модели, которые позволяют довольно легко оценить вероятность и правдоподобность текста.\n",
    "\n",
    "В нграмной языковой модели, нграм - это последовательность из n слов в тексте. Например, в предложении \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\", биграмами будут \"по-моему мы\", \"мы сэкономим\", \"сэкономим уйму\" итд. Языковые модели оценивают вероятность появления последовательности слов, исходя из статистики появления каждого из нграм в обучающей выборке.\n",
    "\n",
    "Порядком (order) нграм языковой модели называют максимальную длину нграм, которую учитывает модель. \n",
    "\n",
    "Практическая работа разделена на 2 части: \n",
    "1. Построение нграмой языковой модели - основная часть, 10 баллов\n",
    "1. Предсказание с помощью языковой модели - дополнительная часть, 6 балла\n",
    "\n",
    "\n",
    "\n",
    "Полезные сслыки:\n",
    "* arpa формат - https://cmusphinx.github.io/wiki/arpaformat/\n",
    "* обучающие материалы - https://pages.ucsd.edu/~rlevy/teaching/2015winter/lign165/lectures/lecture13/lecture13_ngrams_with_SRILM.pdf\n",
    "* обучающие материалы.2 - https://cjlise.github.io/machine-learning/N-Gram-Language-Model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd5c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c1d7",
   "metadata": {},
   "source": [
    "# 1. Построение нграмной языковой модели. (10 баллов)\n",
    "\n",
    "\n",
    "Вероятность текста с помощью нграмной языковой модели можно вычислить по формуле: \n",
    "$$ P(w_1, w_2, .., w_n) = {\\prod{{P_{i=0}^{n}(w_i| w_{i-order}, .., w_{i-1})}}} $$\n",
    "\n",
    "В простом виде, при обучении нграмной языковой модели, чтобы рассчитать условную вероятность каждой нграмы, используется формула, основанная на количестве появлений нграмы в обучающей выборке. Формула выглядит следующим образом:\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i})} \\over {count(w_{i-order},..., w_{i-1})}} $$\n",
    "\n",
    "Поскольку униграмы не содержат в себе какого-дибо контекста, вероятность униграмы можно посчитать поделив кол-во этой слова на общее количество слов в обучающей выборке. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5837fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в первую очередь нам понадобится подсчитать статистику по обучающей выборке \n",
    "def count_ngrams(train_text: List[str], order=3, bos=True, eos=True) -> Dict[Tuple[str], int]:\n",
    "    ngrams = defaultdict(int)\n",
    "    # TODO реализуйте функцию, которая подсчитывает все 1-gram 2-gram ... order-gram ngram'ы в тексте\n",
    "    \n",
    "   \n",
    "    # Проходим по каждому тексту в обучающей выборке\n",
    "    for sentence in train_text:\n",
    "        tokens = sentence.split()  # Разбиваем предложение на токены (слова)\n",
    "        \n",
    "        # Добавляем специальные токены начала и конца, если это указано\n",
    "        if bos:\n",
    "            tokens = ['<s>'] + tokens\n",
    "        if eos:\n",
    "            tokens = tokens + ['</s>']\n",
    "        \n",
    "        # Генерируем n-граммы для всех значений от 1 до указанного порядка\n",
    "        for n in range(1, order + 1):\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                ngram = tuple(tokens[i:i + n])  # Создаем n-грамму\n",
    "                ngrams[ngram] += 1  # Увеличиваем счетчик данной n-граммы\n",
    "    \n",
    "    \n",
    "    # \n",
    "    return dict(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd69d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1a passed\n"
     ]
    }
   ],
   "source": [
    "def test_count_ngrams():\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=True, eos=True) == {\n",
    "        ('<s>',): 1, \n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=True) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1,\n",
    "        ('привет', 'привет'): 1,\n",
    "        ('привет', 'как'): 1,\n",
    "        ('как', 'дела'): 1\n",
    "    }    \n",
    "    assert count_ngrams(['привет ' * 6], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 6, \n",
    "        ('привет', 'привет'): 5\n",
    "    }\n",
    "    result = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие пройдет в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=5)\n",
    "    assert result[('<s>',)] == 3\n",
    "    assert result[('32',)] == 3\n",
    "    assert result[('<s>', 'в', 'офлайне', 'в', '32')] == 1\n",
    "    assert result[('офлайне', 'в', '32', '12', '</s>')] == 1\n",
    "    print('Test 1a passed')\n",
    "    \n",
    "    \n",
    "test_count_ngrams()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e1865",
   "metadata": {},
   "source": [
    "\n",
    "Простой подход к вычислению вероятностей через количество нграм имеет существенный недостаток. Если в тексте встретится нграмма, которой не было в обучающей выборке, то вероятность всего текста будет равна нулю. \n",
    "\n",
    "Чтобы избежать данного недостатка, вводится специальное сглаживание - add-k сглаживание ([Additive, Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)). Данная техника позволяет учитывать нграмы, не встретившиеся в обучающей выборке, и при этом не делает вероятность текста равной нулю.\n",
    "\n",
    "Формула сглаживания Лапласа выглядит следующим образом:\n",
    "\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i}) + k} \\over {count(w_{i-order},..., w_{i-1}) + k*V}} $$\n",
    "\n",
    "Здесь V - количество слов в словаре, а k - гиперпараметр, который контролирует меру сглаживания. Как правило, значение k выбирается экспериментально, чтобы найти оптимальный баланс между учетом редких нграм и сохранением вероятности для часто встречающихся нграм.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4cafb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция подсчета вероятности через количество со сглаживанием Лапласа\n",
    "def calculate_ngram_prob(ngram: Tuple[str], counts: Dict[Tuple[str], int], V=None, k=0) -> float:\n",
    "    # подсчитывет ngram со сглаживанием Лапласа\n",
    "    \n",
    "\n",
    "  # Префикс n-граммы (все слова, кроме последнего)\n",
    "    ngram_count = 0\n",
    "    prefix_count = 0\n",
    "\n",
    "    if len(ngram)>1:\n",
    "      prefix = ngram[:-1]\n",
    "    else:                 #если нграм всего одно слово, расчет усложняется, считаем все случаи когда \n",
    "      prefix = ngram\n",
    "      prefix_count = 0\n",
    "      for gram in counts:\n",
    "        #print(gram)\n",
    "        find_flag = True\n",
    "        i=0\n",
    "        while find_flag and i < len(gram)-1:\n",
    "          #for i in range(0, len(gram)-1):\n",
    "          if gram[i] == xx:\n",
    "            prefix_count += counts.get(gram, 0)\n",
    "            find_flag  = False\n",
    "          i+=1      \n",
    "              \n",
    "      prefix_count = prefix_count + counts.get(('<s>',), 0)\n",
    "      \n",
    "     \n",
    "\n",
    "    # Подсчет частоты самой n-граммы и ее префикса\n",
    "    ngram_count = counts.get(ngram, 0)  # Количество вхождений n-граммы\n",
    "    \n",
    "    if prefix_count == 0:\n",
    "      prefix_count = counts.get(prefix, 0)  # Количество вхождений префикса\n",
    "        \n",
    "    V = len(set([ngram[-1] for ngram in counts.keys()]))\n",
    "    #print(V)\n",
    "    \n",
    "    # Вычисление вероятности со сглаживанием Лапласа\n",
    "    prob = (ngram_count + k) / (prefix_count + k * V)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "60b25d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.b passed\n"
     ]
    }
   ],
   "source": [
    "def test_calculate_ngram_prob():\n",
    "    counts = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=4)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts) == 0.5\n",
    "    assert calculate_ngram_prob(('в', ), counts) == 4/25\n",
    "    assert calculate_ngram_prob(('в', ), counts, k=0.5) == (4+0.5)/(25+0.5*12)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне', 'в', '32'), counts) == 1.0\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=1) == 0.1875\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=0) == 0.0\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=1) == 0.0625\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "\n",
    "    print(\"Test 1.b passed\")\n",
    "    \n",
    "\n",
    "test_calculate_ngram_prob()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da494bf0",
   "metadata": {},
   "source": [
    "Основной метрикой язковых моделей является перплексия. \n",
    "\n",
    "Перплексия  — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку. Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
    "\n",
    "$$ ppl = {P(w_1, w_2 ,..., w_N)^{- {1} \\over {N}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4bd1f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Языковая модель \n",
    "import math\n",
    "\n",
    "class NgramLM:\n",
    "    def __init__(self, order=3, bos=True, eos=True, k=1, predefined_vocab=None):\n",
    "        self.order = order\n",
    "        self.eos = eos\n",
    "        self.bos = bos\n",
    "        self.k = k\n",
    "        self.vocab = predefined_vocab\n",
    "        self.ngrams_count = None\n",
    "        \n",
    "    @property\n",
    "    def V(self) -> int:\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def fit(self, train_text: List[str]) -> None:\n",
    "        # TODO\n",
    "        # Подсчет vocab и ngrams_count по обучающей выборке\n",
    "        # Проходим по каждому тексту в обучающей выборке\n",
    "        ngrams = defaultdict(int)\n",
    "        vocab = set()\n",
    "        \n",
    "        for sentence in train_text:\n",
    "            tokens = sentence.split()  # Разбиваем предложение на токены (слова)\n",
    "            \n",
    "            # Добавляем специальные токены начала и конца, если это указано\n",
    "            if self.bos:\n",
    "                tokens = ['<s>'] + tokens\n",
    "            if self.eos:\n",
    "                tokens = tokens + ['</s>']\n",
    "\n",
    "            vocab.update(tokens)\n",
    "            \n",
    "            # Генерируем n-граммы для всех значений от 1 до указанного порядка\n",
    "            for n in range(1, self.order + 1):\n",
    "                for i in range(len(tokens) - n + 1):\n",
    "                    ngram = tuple(tokens[i:i + n])  # Создаем n-грамму\n",
    "                    ngrams[ngram] += 1  # Увеличиваем счетчик данной n-граммы\n",
    "        \n",
    "        self.ngrams_count = dict(ngrams)\n",
    "        if self.vocab is None:\n",
    "            self.vocab = list(vocab)\n",
    "    \n",
    "                \n",
    "    \n",
    "    def predict_ngram_log_proba(self, ngram: Tuple[str]) -> float:\n",
    "        # TODO \n",
    "        # считаем логарифм вероятности конкретной нграмы\n",
    "        prob = calculate_ngram_prob(ngram, counts = self.ngrams_count, V=self.V, k=self.k)\n",
    "        return math.log(prob)\n",
    "           \n",
    "    def predict_log_proba(self, words: List[str]) -> float:\n",
    "        if self.bos:\n",
    "            words = ['<s>'] + words\n",
    "        if self.eos:\n",
    "            words = words + ['</s>']\n",
    "        logprob = 0\n",
    "        # TODO \n",
    "        # применяем chain rule, чтобы посчитать логарифм вероятности всей строки\n",
    "        for i in range(1, len(words) + 1):\n",
    "            ngram = tuple(words[max(0, i - self.order):i])\n",
    "            logprob += self.predict_ngram_log_proba(ngram)\n",
    "\n",
    "        return logprob\n",
    "        \n",
    "    def ppl(self, text: List[str]) -> float:\n",
    "        #TODO \n",
    "        # подсчет перплексии\n",
    "        # Для того, чтобы ваш код был численно стабильным, \n",
    "        #    не считайте формулу напрямую, а воспользуйтесь переходом к логарифмам вероятностей\n",
    "        log_prob_sum = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for sentence in text:\n",
    "            words = sentence.split()\n",
    "            total_words += len(words)\n",
    "            log_prob_sum += self.predict_log_proba(words)\n",
    "            print(log_prob_sum,total_words)\n",
    "    \n",
    "        perplexity = math.exp(-log_prob_sum / total_words)\n",
    "        #perplexity = abs(log_prob_sum**-(1 / total_words))\n",
    "        # \n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bb0bfe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.2801323269925415 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     ppl \u001b[38;5;241m=\u001b[39m lm\u001b[38;5;241m.\u001b[39mppl(test_data)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mround\u001b[39m(ppl, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m7.33\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mtest_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[160], line 14\u001b[0m, in \u001b[0;36mtest_lm\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mpredict_log_proba([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mпо-моему\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m lm\u001b[38;5;241m.\u001b[39mpredict_log_proba([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mесли\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \n\u001b[0;32m     13\u001b[0m gt \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m41\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m14\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m14\u001b[39m))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m ppl \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m  np\u001b[38;5;241m.\u001b[39misclose(ppl, gt), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m gt \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m41\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m14\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m14\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m14\u001b[39m)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[159], line 81\u001b[0m, in \u001b[0;36mNgramLM.ppl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     78\u001b[0m     log_prob_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_log_proba(words)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(log_prob_sum,total_words)\n\u001b[1;32m---> 81\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlog_prob_sum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#perplexity = abs(log_prob_sum**-(1 / total_words))\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "def test_lm():\n",
    "    train_data = [\"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\",\n",
    "                  \"если я сойду с ума прямо сейчас по-моему мы сэкономим уйму времени\",\n",
    "                  \"мы сэкономим уйму времени если я сейчас сойду с ума по-моему\"]\n",
    "    global lm\n",
    "    lm = NgramLM(order=2)\n",
    "    lm.fit(train_data)\n",
    "    assert lm.V == 14\n",
    "    assert np.isclose(lm.predict_log_proba(['мы']), lm.predict_log_proba([\"если\"]))\n",
    "    assert lm.predict_log_proba([\"по-моему\"]) > lm.predict_log_proba([\"если\"]) \n",
    "    \n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14))**(-1/2)\n",
    "    ppl = lm.ppl([''])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14) * 1/(14)) ** (-1/3)\n",
    "    ppl = lm.ppl(['ЧТО'])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    test_data = [\"по-моему если я прямо сейчас сойду с ума мы сэкономим уйму времени\"]\n",
    "    ppl = lm.ppl(test_data)\n",
    "    assert round(ppl, 2) == 7.33, f\"{ppl}\"\n",
    "test_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafa0a2",
   "metadata": {},
   "source": [
    "# 2. Предсказания с помощью языковой модели (6 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "85d2eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(lm: NgramLM, prefix: List[str], topk=4):\n",
    "    # TODO реализуйте функцию, которая предсказывает продолжение фразы. \n",
    "    # верните topk наиболее вероятных продолжений фразы prefix \n",
    "\n",
    "    candidates = {}\n",
    "    \n",
    "    # Проверяем каждое слово из словаря на возможность продолжить префикс\n",
    "    for word in lm.vocab:\n",
    "        ngram = tuple(prefix[-(lm.order - 1):] + [word])  # Формируем n-грамму из префикса и текущего слова\n",
    "        log_prob = lm.predict_ngram_log_proba(ngram)  # Считаем логарифм вероятности для этой n-граммы\n",
    "        \n",
    "        candidates[word] = log_prob  # Сохраняем вероятность для каждого слова\n",
    "    \n",
    "    # Сортируем кандидатов по вероятности и выбираем top-k\n",
    "    sorted_candidates = sorted(candidates.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    topk_words = [word for word, _ in sorted_candidates[:topk]]  # Возвращаем только слова, не их вероятности\n",
    "    \n",
    "    return topk_words \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4846b",
   "metadata": {},
   "source": [
    "Попробуйте обучить ngram языковую модель на нескольких стихотворениях. Не забудьте трансформировать стихотворение в удобный для ngram модели формат (как сделать так, чтобы модель моделировала рифму?). \n",
    "Попробуйте сгенерировать продолжение для стихотворения с помощью такой языковой модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "34b04750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гений', 'чистой', 'красоты.']\n",
      "['В', 'тревогах', 'шумной', 'суеты,']\n",
      "['Звучал', 'мне', 'долго', 'голос', 'нежный']\n",
      "['И', 'сердце', 'бьется', 'в', 'упоенье,']\n",
      "['красоты.', 'красоты.']\n",
      "['В', 'тревогах', 'шумной', 'суеты,']\n",
      "['Звучал', 'мне', 'долго', 'голос', 'нежный']\n",
      "['И', 'сердце', 'бьется', 'в', 'упоенье,']\n"
     ]
    }
   ],
   "source": [
    "#Your code here \n",
    "\n",
    "text_1 = ['Я помню чудное мгновенье: Передо мной явилась ты,  Как мимолетное виденье, Как гений чистой красоты. В томленьях грусти безнадежной, \\\n",
    "В тревогах шумной суеты, Звучал мне долго голос нежный И снились милые черты.',\n",
    "'Шли годы. Бурь порыв мятежный Рассеял прежние мечты, И я забыл твой голос нежный, Твои небесные черты.',\n",
    "'В глуши, во мраке заточенья Тянулись тихо дни мои Без божества, без вдохновенья, Без слез, без жизни, без любви.',\n",
    "'Душе настало пробужденье: И вот опять явилась ты, Как мимолетное виденье, Как гений чистой красоты.',\n",
    "'И сердце бьется в упоенье, И для него воскресли вновь И божество, и вдохновенье, И жизнь, и слезы, и любовь.',\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "lm_2 = NgramLM(order=4)\n",
    "lm_2.fit(text_1)\n",
    "\n",
    "import random\n",
    "\n",
    "total_pred = []\n",
    "previous_predict = ['']\n",
    "prefix =  ['гений']\n",
    "predict = [prefix[0]]\n",
    "for i in range(0,40):   \n",
    "    prefix = predict_next_word(lm=lm_2, prefix=prefix, topk=1)\n",
    "    \n",
    "    if previous_predict[0] == prefix[0]:                        # если повтор то делаем рандом\n",
    "        \n",
    "        prefix = [total_pred[random.randint(0, len(total_pred)-1)]]\n",
    "        prefix = [predict_next_word(lm=lm_2, prefix=prefix, topk=5)[random.choice([1,2,3,4])]]\n",
    "        \n",
    "        previous_predict = predict\n",
    "        predict = [prefix[0]]\n",
    "    \n",
    "    \n",
    "    if (prefix[0][0].isupper() or prefix[0] == '</s>') and len(predict)>1 : # новая строка для стихотворения\n",
    "        print(predict)\n",
    "        total_pred =  total_pred + predict\n",
    "        previous_predict = predict\n",
    "        predict = []\n",
    "    predict.append(prefix[0])\n",
    "#print(predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
