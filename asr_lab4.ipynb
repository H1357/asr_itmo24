{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceaa7b0d-bc13-4085-90cf-513b0e6c8ddc",
   "metadata": {},
   "source": [
    "# Практическая работа по распознаванию речи #4: <br> Поиск наилучшей гипотезы распознавания\n",
    "Задача распознавания речи состоит в поиске наиболее вероятной словной гипотезы по имеющемуся звуковому сигналу. Используя теорему Байеса, формулировку задачи можно написать так:\n",
    "$$ W^* = \\underset{W}{argmax} {P(W|O)} =  \\underset{W}{argmax} \\sum_i^N{(\\log{P(O|w_i)} + \\log{P(w_i|w_{i-1}, w_{i-2},..))}}$$\n",
    "Где: \n",
    "* $O$ - звук\n",
    "* $W$ - словная гипотеза распознавания \n",
    "* $W^*$ - лучшая гипотеза распознавания\n",
    "* $P(W|O)$ - вероятность гипотезы распознавания при условии наблюдения $O$\n",
    "* $N$ - количество слов в гипотезе\n",
    "* $w_i$ - i'ое слово в гипотезе\n",
    "* $P(O|w_i)$ - акустическое правдоподобие слова (выводится из предсказания акустической моделью)\n",
    "* $P(w_i|w_{i-1}, w_{i-2},..)$ - языковая вероятность слова при условии контекста (предсказывается языковой моделью)\n",
    "\n",
    "\n",
    "В прошлых лабораторных работах были изучены акустическая и языковая модели. Акустическая модель предсказывает вероятность принадлежности кадра некоему акустическому классу (фонеме). Языковая модель предсказывает априорную вероятность последовательности слов. Пришло время разобраться, как соединить эти части в одну систему и получить итоговый результат распознавания. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Основная часть (14 баллов) данной практической работы состоит из двух частей: \n",
    "* знакомство с Weighted Finite State Transducer (WFST)\n",
    "* WFST декодинг в ASR системе\n",
    "\n",
    "### Дополнительная часть \n",
    "* тюнинг параметров beam search (2 балла)\n",
    "\n",
    "## Полезные ссылки: \n",
    "* Наиболее популярная библиотека WFST - [OpenFst](https://www.openfst.org/twiki/bin/view/FST/WebHome)\n",
    "* Библиотека для визуализации - [graphviz](https://graphviz.readthedocs.io/en/stable/manual.html)\n",
    "* Алгоритм обхода графа в ширину - [BFS](https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%85%D0%BE%D0%B4_%D0%B2_%D1%88%D0%B8%D1%80%D0%B8%D0%BD%D1%83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e04b33-5e49-4ecf-8608-6710eb6c24dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "!python --version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fc2f9b-c91b-44d1-b8b2-114a8836ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphviz kenlm kaldiio jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747cd3cb-0d09-4ecb-8468-7341fc70b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import abc\n",
    "\n",
    "from typing import List, Dict, Union, Set, Any, Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import graphviz\n",
    "import kenlm \n",
    "import jiwer\n",
    "from kaldiio import ReadHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc3ee2-63c9-4fdb-a7ab-3fbace5043d4",
   "metadata": {},
   "source": [
    "# 1. Weighted Finite State Transducer (WFST)\n",
    "\n",
    "\n",
    "\n",
    "Из определения в wiki [Weighted Finite State Transducer (Взвешенный конечный автомат с выходом)](https://en.wikipedia.org/wiki/Finite-state_transducer) следует, что главная задача FST - это переводить символы из входного алфавита в соответствующие им символы из выходного алфавита. Например, с помощью FST можно перевести последовательность фонем в последовательность слов. Конечный автомат будет менять свое состояние при обработке последовательности входных символов (фонем). Когда он соберет из входных фонем корректную транскрипцию слова, то выдаст это слово на выход. \n",
    "\n",
    "Однако не всегда можно однозначно перевести последовательность фонем в слова (некоторые слова произносятся одинаково, а пишутся по-разному), поэтому для распознавания речи надо использовать не простой FST, а взвешенный. WFST позволяет не только перевести входные символы в выходные, но и оценить вес такого перевода. Чем больше вес, тем хуже гипотеза.  \n",
    "\n",
    "Напишем собственную реализацию WFST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f84131-244f-4672-bb95-f245385b6a88",
   "metadata": {},
   "source": [
    "### AbstractWFST\n",
    "AbstractWFST -- это базовый интерфейс нашего WFST. В нем заданы основные методы, через которые мы будем работать с wfst  \n",
    "* get_start - возвращаяет стартовое состояние конечного автомата.\n",
    "* final_score - вес завершения работы в данном состоянии. В некоторых состояниях завершить обработку невозможно, в таких случаях final weight  будет равен бесконечности.\n",
    "*  transduce - совершает переход из текущего состояния в следующее, соответствующее переходу по символу ilabel. Возвращает выходной символ, вес перехода и следующее состояние.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86425820-9a05-4463-8c97-5d749dc43f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractWFST(abc.ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_start(self) -> Any:\n",
    "        \"\"\"Return start state\"\"\"\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def final_score(self, state: Any) -> float:\n",
    "        \"\"\"is the state final? If true return negative log likelihood of the finalization. otherwise return inf\"\"\"\n",
    "           \n",
    "    @abc.abstractmethod\n",
    "    def transduce(self, state: Any, ilabel: str) -> Tuple[Tuple[str, float, Any]]:\n",
    "        \"\"\"Transduce ilabel to olabel. \n",
    "        return all available olabels for this state and ilabel pair\n",
    "        return type - ((olabel1, weight1, nextstate1), \n",
    "                       ((olabel1, weight1, nextstate1), \n",
    "                       ...)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df77d5-b466-4eb4-8179-51ed23b7b0db",
   "metadata": {},
   "source": [
    "### SymbolsMap\n",
    "\n",
    "Для того, чтобы задать WFST, первым делом нам понадобятся таблицы входных и выходных символов. Для задания таблиц будем использовать класс SymbolsMap. Данный класс служит для маппинга символов на индексы и обратно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f4441149-62c8-4255-b240-4652f8ff9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolsMap:\n",
    "    def __init__(self, id2symbol: Optional[Dict[int, str]] = None, symbol2id: Optional[Dict[str, int]] = None):\n",
    "        assert id2symbol is not None or symbol2id is not None, f\"One id2symbol or symbol2id must be not None\"\n",
    "        assert id2symbol is None or symbol2id is None, f\"Only one One id2symbol or symbol2id can be not None\"\n",
    "        if id2symbol is None:\n",
    "            self.id2symbol = {i:s for s,i in symbol2id.items()}\n",
    "        elif isinstance(id2symbol, dict):\n",
    "            self.id2symbol = id2symbol\n",
    "        elif isinstance(id2symbol, list):\n",
    "            self.id2symbol = {i:s for i, s in enumerate(id2symbol)}\n",
    "        else:\n",
    "            raise RuntimeError(f\"unknown type {type(id2symbol)=}\")\n",
    "            \n",
    "        if symbol2id is None:\n",
    "            self.symbol2id = {s:i for i, s in self.id2symbol.items()}\n",
    "        else:\n",
    "            self.symbol2id = symbol2id   \n",
    "                \n",
    "        assert self.id2symbol[0] == '<eps>', f\"wrong {self.id2symbol}\"\n",
    "\n",
    "    def get_id(self, symbol: str):\n",
    "        #print(self)\n",
    "        #print(symbol)\n",
    "        \n",
    "        return self.symbol2id[symbol]\n",
    "        \n",
    "    def get_symbol(self, id: int):\n",
    "        return self.id2symbol[id] \n",
    "    \n",
    "    def get_all_symbols(self):\n",
    "        print(self.id2symbol)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, fname):\n",
    "        \"\"\"read symbols table from file\n",
    "        format: \n",
    "            word id\n",
    "            word2 id2\n",
    "            ...\n",
    "        \"\"\"\n",
    "        with open(fname) as f:\n",
    "            s2i = {s:int(i) for s, i in map(str.split, f.readlines())}\n",
    "        return cls(symbol2id=s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6a626-366b-4ff9-ba63-0c2f03818367",
   "metadata": {},
   "source": [
    "### Arc и ILabelIndexedArcs\n",
    "Переходы внутри FST будем описывать с помощью класса, инкапсулирующего информацию о входном/выходном индексе символа, весе перехода и следующем состоянии FST. Поскольку дуги мы будем хранить отдельно для каждого состояния, информация о текущем состоянии в дуге не нужна. \n",
    "\n",
    "Для быстрого выбора нужной дуги создадим специальную коллекцию ILabelIndexedArcs. Данная коллекция хранит дуги таким образом, чтобы выбор дуги по входному символу осуществлялся за O(1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a098c7d-1d87-4e65-81c8-43a18e4df4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arc:\n",
    "    \"\"\"Arc in WFST\"\"\"\n",
    "    ilabel: int\n",
    "    olabel: int\n",
    "    weight: float\n",
    "    nextstate: Any\n",
    "\n",
    "class ILabelIndexedArcs:\n",
    "    def __init__(self, arcs: Optional[List[Arc]] = None):\n",
    "        self.ilabel2arclist = defaultdict(list)\n",
    "        if arcs is not None:\n",
    "            for arc in arcs:\n",
    "                self.add_arc(arc)\n",
    "\n",
    "    def add_arc(self, arc: Arc):\n",
    "        self.ilabel2arclist[arc.ilabel].append(arc)\n",
    "\n",
    "    def get_arcs_by_ilabel(self, ilabel: int):\n",
    "        return self.ilabel2arclist[ilabel]\n",
    "\n",
    "    def arcs(self) -> List[Arc]:\n",
    "        \"\"\"Return all arcs\"\"\"\n",
    "        all_arcs = []\n",
    "        # TODO\n",
    "        # реализуйте функцию, которая возвращает все дуги, хранящиеся в данной коллекции\n",
    "        #raise NotImplementedError()\n",
    "        for arclist in self.ilabel2arclist.values():\n",
    "            all_arcs.extend(arclist)\n",
    "        return all_arcs\n",
    "        \n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ILabelIndexedArcs([\" + \", \".join(f\"{a}\" for a in self.arcs()) + \"])\"\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bc0d83-fecf-42e6-9dd1-4e95622cc6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.a passed\n"
     ]
    }
   ],
   "source": [
    "# test 1.a\n",
    "def test_ILabelIndexedArcs():\n",
    "    # проверка что arcs работает правильно \n",
    "    arcs = [Arc(i, -i, i/100, i+100) for i in range(10)]\n",
    "    c = ILabelIndexedArcs(arcs)\n",
    "    arcs2 = c.arcs()\n",
    "    \n",
    "    assert len(arcs) == len(arcs2), f\"{len(arcs)=}, {len(arcs2)=}\\n{arcs} != {c.arcs()}\"\n",
    "    for a in arcs2:\n",
    "        assert a in arcs, f\"{a=} not in {arcs=}\"\n",
    "    print('Test 1.a passed')\n",
    "test_ILabelIndexedArcs()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d7679-8e0c-41d8-8ac5-bdd38841594f",
   "metadata": {},
   "source": [
    "### WFST \n",
    "Все готово для создания взвешенного конечного автомата. Класс WFST состоит из таблицы входных и выходных символов, списка состояний и коллекции дуг для каждого, а также множества финальных состояний. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb8dab12-e963-44c8-98a1-cde9f8044c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFST(AbstractWFST):\n",
    "    def __init__(self, \n",
    "                 isymbols: Optional[SymbolsMap] = None, \n",
    "                 osymbols: Optional[SymbolsMap] = None):\n",
    "        self.start = 0\n",
    "        self.states = [0]\n",
    "        # состояния, в которых может завершиться декодирование без какого либо штрафа \n",
    "        self.final_states = set() \n",
    "        self.isymbols = isymbols\n",
    "        self.osymbols = osymbols\n",
    "        self.state2arcs = defaultdict(ILabelIndexedArcs)\n",
    "\n",
    "    def get_start(self):\n",
    "        \"\"\"return start state\"\"\"\n",
    "        return self.start\n",
    "        \n",
    "    def final_score(self, state_id: int):\n",
    "        \"\"\"return the weight of decoding completion in state_id\"\"\"\n",
    "        # TODO \n",
    "        # верните вес завершения декодирования в state_id \n",
    "        if state_id in self.final_states:\n",
    "            return 0.0  # No penalty for final states\n",
    "        else:\n",
    "            return float('inf')  # Non-final states have an infinite weight\n",
    "       \n",
    "    def transduce(self, state, ilabel: str) -> Tuple[Tuple[str, float, int]]:\n",
    "        \"\"\"transitions wfst to next state by 'ilabel' input symbol, except when the input symbol is <eps>.\n",
    "        Returns all possible output results as a tuple of triples (olabel, weight, nextstate).\"\"\"\n",
    "        assert self.isymbols is not None and self.osymbols is not None, f\"Cannot transduce w/o both symbols tables\" \n",
    "        \n",
    "\n",
    "        if ilabel == '<eps>':\n",
    "            # skip <eps> input\n",
    "            return ((ilabel, 0, state), )\n",
    "        \n",
    "        #print('ilabel_',ilabel)\n",
    "        #self.isymbols.get_all_symbols()\n",
    "        #self.osymbols.get_all_symbols()\n",
    "        \n",
    "        label_id = self.isymbols.get_id(ilabel)\n",
    "        \n",
    "        # TODO \n",
    "        # верните все возможные результаты перехода из текущего состояния по входу ilabel\n",
    "        # результат должен выглядеть так: tuple((\"слово\", 0.1, 2), (\"другоеслово\", 10, 4), ...)\n",
    "        \n",
    "        \n",
    "        # Retrieve all arcs for the given input label ID in the current state\n",
    "        arcs = self.state2arcs[state].get_arcs_by_ilabel(label_id)\n",
    "        \n",
    "        # Prepare the result tuple with all possible transitions\n",
    "             \n",
    "        return tuple((self.osymbols.get_symbol(arc.olabel), arc.weight, arc.nextstate) for arc in arcs)\n",
    "    \n",
    "    def set_final(self, state: int):\n",
    "        \"\"\"sets the final weight for the state to zero\"\"\"\n",
    "        # TODO добавьте возможность завершать декодирование в state\n",
    "        self.final_states.add(state)\n",
    "        \n",
    "    def new_state(self):\n",
    "        \"\"\"Create new state id and return it\"\"\"\n",
    "        # TODO \n",
    "        # добавьте в конечный автомат новое состояние и верните его id \n",
    "        \n",
    "        # Generate a new state ID as the next integer\n",
    "        state_id = len(self.states)\n",
    "        # Add the new state to the list of states\n",
    "        self.states.append(state_id)\n",
    "        return state_id\n",
    "    \n",
    "    def add_arc(self, state_from: int, arc: Arc):\n",
    "        \"\"\"adds a new arc for this state\"\"\"\n",
    "        # TODO \n",
    "        # добавьте в конечный автомат новых переход arc, выходящий из state_from \n",
    "        \n",
    "        # Add the arc to the list of arcs for the given state\n",
    "        self.state2arcs[state_from].add_arc(arc)\n",
    "        \n",
    "    \n",
    "    def to_dot(self):\n",
    "        \"\"\"Visualize the WFST\"\"\" \n",
    "        dot = graphviz.Digraph()\n",
    "        for s in self.states:\n",
    "            dot.node(str(s))\n",
    "        for state_from, arcs_container in self.state2arcs.items():\n",
    "            for arc in arcs_container.arcs():\n",
    "                il = self.isymbols.get_symbol(arc.ilabel) if self.isymbols is not None else arc.ilabel\n",
    "                ol = self.osymbols.get_symbol(arc.olabel) if self.osymbols is not None else arc.olabel\n",
    "                dot.edge(str(state_from), str(arc.nextstate), label=f\"{il}:{ol}:{arc.weight:.2f}\")\n",
    "        return dot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b8959ca8-4e58-4ea2-b4c9-d4ce12d800cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1.b passed\n"
     ]
    }
   ],
   "source": [
    "# test 1.b\n",
    "def test_WFST():\n",
    "    en_l = SymbolsMap(id2symbol=['<eps>', 'b', 'd', 'u'])\n",
    "    ru_l = SymbolsMap(id2symbol=['<eps>', 'ю', 'б', 'д'])\n",
    "    \n",
    "    # create first transducer\n",
    "    en2ru = WFST(isymbols=en_l, osymbols=ru_l)\n",
    "    # new_state\n",
    "    en2ru_final = en2ru.new_state()\n",
    "    assert en2ru_final == 1 , f'start - zero, next state - one, next - two ...'\n",
    "    assert en2ru.new_state() == 2, f'start - zero, next state - one, next - two ...'\n",
    "    en2ru.set_final(en2ru_final)\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(1, 2, 1.0, en2ru_final))\n",
    "    en2ru.add_arc(en2ru_final, Arc(2, 3, 2.0, en2ru.get_start()))\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(3, 1, 3.0, en2ru_final))\n",
    "    # you can use this line to visualize\n",
    "    #display.display(en2ru.to_dot())\n",
    "\n",
    "    # final_score # set_final\n",
    "    assert en2ru.final_score(en2ru.get_start()) == float('inf')\n",
    "    assert en2ru.final_score(en2ru_final) == 0\n",
    "    en2ru.set_final(en2ru.get_start())\n",
    "    assert en2ru.final_score(en2ru_final) == en2ru.final_score(en2ru.get_start()) == 0\n",
    "\n",
    "        \n",
    "    # transduce # add_arc\n",
    "    assert en2ru.transduce(en2ru.get_start(), 'd') == tuple() , \"Cannot transduce 'd' from start state. output must be zero len tuple\"\n",
    "    assert en2ru.transduce(en2ru.get_start(), 'b') == (('б', 1.0, en2ru_final), ) , \"Arc(1, 2, 1.0, en2ru_final)\"\n",
    "    assert en2ru.transduce(en2ru.get_start(), 'u') == (('ю', 3.0, en2ru_final), ) , \"Arc(3, 1, 3.0, en2ru_final)\"\n",
    "    assert en2ru.transduce(en2ru_final, 'd') == (('д', 2.0, en2ru.get_start()), ) , \"Arc(2, 3, 2.0, en2ru.get_start())\"\n",
    "    \n",
    "    print('test 1.b passed')\n",
    "test_WFST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598f04d-11fe-4b37-a396-1eea00d351bd",
   "metadata": {},
   "source": [
    "## wfst композиция \n",
    "\n",
    "\n",
    "Над WFST определен большой набор различных операций, реализация большинства из которых является сложной алгоритмической задачей. Важной для распознавания речи операцией является [композиция](https://www.openfst.org/twiki/bin/view/FST/ComposeDoc) набора различных wfst. \n",
    "\n",
    "OpenFST дает следующее определение композиции:\n",
    " \n",
    "\"This operation computes the composition of two transducers. If A transduces string x to y with weight a and B transduces y to z with weight b, then their composition transduces string x to z with weight a ⊗ b.\"\n",
    "\n",
    "Другими словами, результатом композиции является wfst, применение которого к входной последовательности даст такой же результат, как и последовательное применение композируемых wfst. \n",
    "\n",
    "Хорошим примером композиции различных WFST является граф распознавания в гибридных системах распознавания речи. Такой граф является результатом композиции четырех wfst:\n",
    "* H - Hidden Markov Model wfst (переводит акустические классы, предсказанные с помощью AM, в трифоны)\n",
    "* С - Context-dependency transducer (переводит трифоны (тройки (leftcontext,phone,rightcontext)) в фонемы)\n",
    "* L - lexicon (переводит фонемы в слова)\n",
    "* G - ngram lm (оценивает вероятность последовательностей слов)\n",
    " \n",
    "Итого граф распознавания можно выразить с помощью следующей формулы:\n",
    "$$ HCLG = H⊗C⊗L⊗G $$ \n",
    "где ⊗ - оператор композиции двух wfst. Подробнее про граф распознавания можно почитать в [документации к фреймворку kaldi](https://kaldi-asr.org/doc/graph.html)\n",
    "\n",
    "\n",
    "Композиция позволяет объединить много обработчиков в один большой граф, что несомненно является большим плюсом для построения продакшн решений. Но у такого подхода есть и минусы - процесс подготовки графа очень сложен и требует множества оптимизаций. Это усложняет любые эксперименты и модификации системы. В данной работе мы не будем реализовывать честную композицию графа, а будем считать композицию \"On the Fly\". То есть сделаем обертку, реализующую интерфейс AbstractWFST, состоянием которой будет Tuple состояний всех композируемых wfst, а метод transduce будет последовательно проходить через эти wfst. Тем самым, по определению композиции, наш класс будет эквивалентен честной композиции.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a52b7dc4-b98d-468a-9d61-990eda9217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnTheFlyCompose(AbstractWFST):\n",
    "    def __init__(self, wfsts: List[AbstractWFST]):\n",
    "        # !!!Attention!!! wfsts - это лист AbstractWFST, а не WFST. \n",
    "        # Для работы с wfsts истользуйте только методы из AbstractWFST\n",
    "        self.wfsts = wfsts\n",
    "\n",
    "    def get_start(self):\n",
    "        return tuple(fst.get_start() for fst in self.wfsts)\n",
    "\n",
    "    def final_score(self, state: Tuple[Any]):\n",
    "        assert len(state) == len(self.wfsts)\n",
    "        # TODO \n",
    "        # посчитайте финальный скор для state\n",
    "        # результат композиции может завершить обработку только в тех стейтах, где все wfsts будут в финальных состояниях\n",
    "        \n",
    "        # Calculate the final score by checking if each FST's state is final\n",
    "        final_scores = []\n",
    "        for fst, s in zip(self.wfsts, state):\n",
    "            \n",
    "            #print(fst)\n",
    "            if fst.final_score(s) != float('inf'):\n",
    "                # Get the final score if this state's FST can terminate here\n",
    "                final_scores.append(fst.final_score(s))\n",
    "            else:\n",
    "                # If any FST state is not final, the composition cannot terminate\n",
    "                return float('inf')\n",
    "           \n",
    "    \n",
    "        \n",
    "        # Return the combined final score, summing all individual final scores\n",
    "        return sum(final_scores)\n",
    "    \n",
    "    def _transduce(self, state: Tuple[Any], ilabel: str) -> List[Tuple[List[str], float, List[int]]]:\n",
    "        assert len(self.wfsts) == len(state)\n",
    "        \n",
    "        # Найдите все возможные гипотезы перевода ilabel с помощью композиции всех self.wfsts\n",
    "        # Последовательно пройдите через все self.wfsts, расширяя список гипотез и дополняя гипотезы новыми слоями \n",
    "        # Верните результат в виде списка гипотез. \n",
    "        # Каждая гипотеза содержит три элемента: \n",
    "        #   0. список символов со ВСЕХ прошедших слоев композиции \n",
    "        #   1. суммарный вес гипотезы \n",
    "        #   2. список состояний, в которые перешли self.wfsts \n",
    "\n",
    "        # Одна стартовая гипотеза. Корень для всех гипотез.\n",
    "        # Еще никакой wfst не применен, символ только ilabel, вес стартовый, стейтов еще нет. \n",
    "        \n",
    "        hyps_per_layer = [([ilabel], 0, [])]  \n",
    "        for fst, s in zip(self.wfsts, state):\n",
    "            new_hyps = []\n",
    "            # TODO \n",
    "            # примените fst.transduce ко всем гипотезам из hyps_per_layer\n",
    "            # постепенно разветвляя их и сохраняя в new_hyp\n",
    "            # на вход подайте выход последнего из уже обработаных слоев \n",
    "            # Process each hypothesis in hyps_per_layer through the current WFST layer\n",
    "            for symbols, weight, states in hyps_per_layer:\n",
    "                # Get all arcs (possible transitions) for the current WFST and state `s`\n",
    "                \n",
    "                #arcs = fst.transduce(s, ilabel)\n",
    "                arcs = fst.transduce(s, ilabel)\n",
    "                \n",
    "                for olabel, arc_weight, nextstate in arcs:\n",
    "                    # Generate a new hypothesis\n",
    "                    new_symbols = symbols + [olabel]\n",
    "                    new_weight = weight + arc_weight\n",
    "                    new_states = states[:]\n",
    "\n",
    "                    #print(self.wfsts.index(fst))\n",
    "                    #print(new_states)\n",
    "                    #print(nextstate)\n",
    "                    if self.wfsts.index(fst) == 0:\n",
    "                        \n",
    "                        new_states.append(nextstate)\n",
    "                        #print(new_states)\n",
    "                    else:\n",
    "                        new_states[self.wfsts.index(fst)] = nextstate  # Update state for this layer\n",
    "\n",
    "                    # Add the expanded hypothesis to new_hyps\n",
    "                    new_hyps.append((new_symbols, new_weight, new_states))\n",
    "                print(new_hyps)\n",
    "            # Move to the next WFST layer with the newly expanded hypotheses\n",
    "            hyps_per_layer = new_hyps\n",
    "            \n",
    "        return hyps_per_layer\n",
    "\n",
    "    def transduce(self, state: Tuple[Any], ilabel: str):\n",
    "        \n",
    "        \n",
    "        hyps_per_layer = self._transduce(state, ilabel)\n",
    "        \n",
    "        \n",
    "        # выходной символ transduce в композиции - это выходной символ самого последнего wfst \n",
    "        return tuple((ls[-1], w, tuple(ss)) for ls, w, ss in hyps_per_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "de383686-63f9-4664-bceb-e6eed2f1c1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.0 (20241103.1931)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"196pt\" height=\"133pt\"\n",
       " viewBox=\"0.00 0.00 196.09 132.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 128.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-128.5 192.09,-128.5 192.09,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"70.34\" cy=\"-106.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"70.34\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94.88,-114.67C105.85,-115.45 115.34,-112.72 115.34,-106.5 115.34,-102.61 111.64,-100.09 106.18,-98.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.56,-95.45 96.39,-98.41 106.19,-102.44 106.56,-95.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.72\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d:&lt;eps&gt;:0.10</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"70.34\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"70.34\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.07,-99.36C30.12,-94.29 12.39,-85.43 3.34,-70.5 -7.32,-52.9 13.83,-39.12 35.12,-30.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"36.26,-33.57 44.36,-26.74 33.77,-27.03 36.26,-33.57\"/>\n",
       "<text text-anchor=\"middle\" x=\"25.84\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b:б:1.00</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M70.34,-88.41C70.34,-76.76 70.34,-61.05 70.34,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"73.84,-47.86 70.34,-37.86 66.84,-47.86 73.84,-47.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"94.72\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">u:ю:3.00</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.05,-28.91C109.31,-38.3 129.21,-53.48 120.34,-70.5 116.22,-78.43 109.38,-84.93 102.06,-90.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.64,-86.86 94,-95.11 104.34,-92.8 100.64,-86.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"145.02\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d:д:2.00</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1dd1d8be230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['b', 'б'], 1.0, [1])]\n",
      "[(['d', '<eps>'], 0.1, [0])]\n",
      "[(['u', 'ю'], 3.0, [1])]\n",
      "[]\n",
      "[(['d', 'д'], 2.0, [0])]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.0 (20241103.1931)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"204pt\" height=\"133pt\"\n",
       " viewBox=\"0.00 0.00 204.30 132.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 128.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-128.5 200.3,-128.5 200.3,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"79.39\" cy=\"-106.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.39\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"79.39\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.39\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.47,-101C36.06,-96.49 14.36,-87.63 3.39,-70.5 -9.27,-50.74 17.77,-36.68 43.03,-28.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"44,-31.65 52.54,-25.38 41.95,-24.96 44,-31.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"30.39\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">б:B:10.00</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79.39,-36.35C79.39,-48.06 79.39,-63.79 79.39,-77.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"75.89,-76.9 79.39,-86.9 82.89,-76.9 75.89,-76.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.76\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">д:D:30.00</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.17,-27.84C121.8,-36.91 145.51,-52.25 135.39,-70.5 130.34,-79.61 121.83,-86.66 112.94,-91.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.48,-88.78 104.26,-96.53 114.74,-94.97 111.48,-88.78\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.05\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ю:U:40.00</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M103.92,-26.17C114.9,-26.95 124.39,-24.22 124.39,-18 124.39,-14.11 120.68,-11.59 115.22,-10.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.6,-6.95 105.43,-9.91 115.23,-13.94 115.6,-6.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.76\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">д:D:20.00</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1dd2139bf70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['b', 'б'], 1.0, [1])]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 67\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m your_score \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myour_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, correct = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest 1.c passed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mtest_OnTheFlyCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[107], line 49\u001b[0m, in \u001b[0;36mtest_OnTheFlyCompose\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# композиция двух работает как последовательное применение каждого \u001b[39;00m\n\u001b[0;32m     47\u001b[0m comp \u001b[38;5;241m=\u001b[39m OnTheFlyCompose([en2ru, ru2en])\n\u001b[1;32m---> 49\u001b[0m hyps \u001b[38;5;241m=\u001b[39m \u001b[43mcomp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hyps \u001b[38;5;241m==\u001b[39m ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m11.0\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), ) , \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyps\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m hyps \u001b[38;5;241m=\u001b[39m comp\u001b[38;5;241m.\u001b[39mtransduce((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[106], line 88\u001b[0m, in \u001b[0;36mOnTheFlyCompose.transduce\u001b[1;34m(self, state, ilabel)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: Tuple[Any], ilabel: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 88\u001b[0m     hyps_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43milabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# выходной символ transduce в композиции - это выходной символ самого последнего wfst \u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m((ls[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], w, \u001b[38;5;28mtuple\u001b[39m(ss)) \u001b[38;5;28;01mfor\u001b[39;00m ls, w, ss \u001b[38;5;129;01min\u001b[39;00m hyps_per_layer)\n",
      "Cell \u001b[1;32mIn[106], line 59\u001b[0m, in \u001b[0;36mOnTheFlyCompose._transduce\u001b[1;34m(self, state, ilabel)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# TODO \u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# примените fst.transduce ко всем гипотезам из hyps_per_layer\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# постепенно разветвляя их и сохраняя в new_hyp\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# на вход подайте выход последнего из уже обработаных слоев \u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Process each hypothesis in hyps_per_layer through the current WFST layer\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbols, weight, states \u001b[38;5;129;01min\u001b[39;00m hyps_per_layer:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Get all arcs (possible transitions) for the current WFST and state `s`\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m#arcs = fst.transduce(s, ilabel)\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     arcs \u001b[38;5;241m=\u001b[39m \u001b[43mfst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43milabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m olabel, arc_weight, nextstate \u001b[38;5;129;01min\u001b[39;00m arcs:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# Generate a new hypothesis\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         new_symbols \u001b[38;5;241m=\u001b[39m symbols \u001b[38;5;241m+\u001b[39m [olabel]\n",
      "Cell \u001b[1;32mIn[96], line 40\u001b[0m, in \u001b[0;36mWFST.transduce\u001b[1;34m(self, state, ilabel)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ((ilabel, \u001b[38;5;241m0\u001b[39m, state), )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#print('ilabel_',ilabel)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#self.isymbols.get_all_symbols()\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#self.osymbols.get_all_symbols()\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m label_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misymbols\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43milabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# TODO \u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# верните все возможные результаты перехода из текущего состояния по входу ilabel\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# результат должен выглядеть так: tuple((\"слово\", 0.1, 2), (\"другоеслово\", 10, 4), ...)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Retrieve all arcs for the given input label ID in the current state\u001b[39;00m\n\u001b[0;32m     48\u001b[0m arcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate2arcs[state]\u001b[38;5;241m.\u001b[39mget_arcs_by_ilabel(label_id)\n",
      "Cell \u001b[1;32mIn[73], line 25\u001b[0m, in \u001b[0;36mSymbolsMap.get_id\u001b[1;34m(self, symbol)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, symbol: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#print(self)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#print(symbol)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbol2id\u001b[49m\u001b[43m[\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'b'"
     ]
    }
   ],
   "source": [
    "def test_OnTheFlyCompose():\n",
    "    en_l = SymbolsMap(id2symbol=['<eps>', 'b', 'd', 'u'])\n",
    "    ru_l = SymbolsMap(id2symbol=['<eps>', 'ю', 'д', 'б'])\n",
    "    en_U = SymbolsMap(id2symbol=['<eps>', 'B', 'D', 'U'])\n",
    "    ru_U = SymbolsMap(id2symbol=['<eps>', 'Ю', 'Д', 'Б'])\n",
    "    \n",
    "    # create first transducer\n",
    "    en2ru = WFST(isymbols=en_l, osymbols=ru_l)\n",
    "    en2ru_final = en2ru.new_state()\n",
    "    en2ru.set_final(en2ru_final)\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(1, 3, 1.0, en2ru_final))\n",
    "    en2ru.add_arc(en2ru_final, Arc(2, 2, 2.0, en2ru.get_start()))\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(2, 0, 0.1, en2ru.get_start()))\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(3, 1, 3.0, en2ru_final))\n",
    "    # you can use this line to visualize\n",
    "    display.display(en2ru.to_dot())\n",
    "\n",
    "    # OnTheFlyCompose одного fst работает так же как и этот fst\n",
    "    comp = OnTheFlyCompose([en2ru])\n",
    "    for s in (0, 1):\n",
    "        for il in 'bdu':\n",
    "            hyps1 = en2ru.transduce(s, il)\n",
    "            hyps2 = comp.transduce((s,), il)\n",
    "            for h in hyps2:\n",
    "                # only one state\n",
    "                assert len(h[2]) == 1, f\"{hyps2=}\"\n",
    "            hyps2_flatten = tuple((l, w, ss[0]) for l,w,ss in hyps2)\n",
    "            assert hyps1 == hyps2_flatten, f\"{hyps1=} {hyps2_flatten=}\"\n",
    "        \n",
    "        scores = [float('inf'), 0]\n",
    "        your_score = comp.final_score((s,))\n",
    "        assert your_score == scores[s], f\"Your score = {your_score}, correct = {scores[s]}\"\n",
    "\n",
    "    # create second transducer\n",
    "    ru2en = WFST(isymbols=ru_l, osymbols=en_U)\n",
    "    ru2en_final = ru2en.new_state()\n",
    "    ru2en.set_final(ru2en_final)\n",
    "    ru2en.add_arc(ru2en.get_start(), Arc(3, 1, 10.0, ru2en_final))\n",
    "    ru2en.add_arc(ru2en_final, Arc(2, 2, 20.0, ru2en_final))\n",
    "    ru2en.add_arc(ru2en_final, Arc(2, 2, 30.0, ru2en.get_start()))\n",
    "    ru2en.add_arc(ru2en_final, Arc(1, 3, 40.0, ru2en.get_start()))\n",
    "    \n",
    "    # you can use this line to visualize\n",
    "    display.display(ru2en.to_dot())\n",
    "\n",
    "    # композиция двух работает как последовательное применение каждого \n",
    "    comp = OnTheFlyCompose([en2ru, ru2en])\n",
    "    \n",
    "    hyps = comp.transduce((0, 0), 'b')\n",
    "    assert hyps == (('B', 11.0, (1, 1)), ) , f\"{hyps =}\"\n",
    "    hyps = comp.transduce((1, 1), 'd')\n",
    "    assert hyps == (('D', 22.0, (0, 1)), ('D', 32.0, (0, 0))) , f\"{hyps =}\"\n",
    "    hyps = comp.transduce((0, 1), 'd')\n",
    "    assert hyps == (('<eps>', 0.1, (0, 1)), ) , f\"{hyps =}\"\n",
    "    # Проверка правильности расчетов final_score\n",
    "    your_score = comp.final_score((0, 0))\n",
    "    assert your_score == float('inf'), f\"Your score = {your_score}, correct = {float('inf')}\"\n",
    "    your_score = comp.final_score((0, 1))\n",
    "    assert your_score == float('inf'), f\"Your score = {your_score}, correct = {float('inf')}\"\n",
    "    your_score = comp.final_score((1, 0))\n",
    "    assert your_score == float('inf'), f\"Your score = {your_score}, correct = {float('inf')}\"\n",
    "    your_score = comp.final_score((1, 1))\n",
    "    assert your_score == 0, f\"Your score = {your_score}, correct = {0}\"\n",
    "    print('Test 1.c passed')\n",
    "    \n",
    "    \n",
    "test_OnTheFlyCompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615649a8-ea1b-4d59-be37-1a18fb509619",
   "metadata": {},
   "source": [
    "# 2. WFST декодинг в ASR системе\n",
    "Отлично, код WFST готов, теперь необходимо собрать граф распознавания и написать поиск наилучшей гипотезы \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2cba40b2-2a44-47d2-897f-f26df9dfb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала подготовим нужные нам таблицы символов\n",
    "AM_PHONES = {0: 'pau', 1: 'aa', 2: 'ae', 3: 'ah', 4: 'ao', 5: 'aw', 6: 'ax', 7: 'ax-h', 8: 'axr', 9: 'ay', 10: 'b', \n",
    "             11: 'bcl', 12: 'ch', 13: 'd', 14: 'dcl', 15: 'dh', 16: 'dx', 17: 'eh', 18: 'el', 19: 'em', 20: 'en', \n",
    "             21: 'eng', 22: 'er', 23: 'ey', 24: 'f', 25: 'g', 26: 'gcl', 27: 'hh', 28: 'hv', 29: 'ih', 30: 'ix', \n",
    "             31: 'iy', 32: 'jh', 33: 'k', 34: 'kcl', 35: 'l', 36: 'm', 37: 'n', 38: 'ng', 39: 'nx', 40: 'ow', \n",
    "             41: 'oy', 42: 'p', 43: 'pcl', 44: 'q', 45: 'r', 46: 's', 47: 'sh', 48: 't', 49: 'tcl', 50: 'th', \n",
    "             51: 'uh', 52: 'uw', 53: 'ux', 54: 'v', 55: 'w', 56: 'y', 57: 'z', 58: 'zh'}\n",
    "\n",
    "# таблица фонем для wfst \n",
    "def create_phones_txt(fname='exp/phones.txt'):\n",
    "    fname = Path(fname) \n",
    "    fname.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('<eps> 0\\n') # !! shift all ids by one !! \n",
    "        for i, p in sorted(AM_PHONES.items()):\n",
    "            f.write(f'{p} {i+1}\\n')\n",
    "create_phones_txt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "565bc3b5-d319-481d-bf69-01251746fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим подготовленный лексикон из датасета \n",
    "def load_lexicon_file(fname='timit/TIMITDIC.TXT', words_limit=-1):\n",
    "    \"\"\"generator running through the 'fname' phonetic dictionary \n",
    "    yield (word: str, phones: List[str])\"\"\"\n",
    "    num = 0\n",
    "    with open(fname) as f:\n",
    "        for line in map(str.strip, f.readlines()):\n",
    "            if line.startswith(';'):\n",
    "                continue\n",
    "            word, trans, _ = line.split('/')\n",
    "            # remove ~adj suffix \n",
    "            word = word.split('~')[0].strip() \n",
    "            # remove stress factor\n",
    "            trans = [t[:-1] if t[-1].isdigit() else t for t in trans.split()]\n",
    "            yield word, trans\n",
    "            num += 1\n",
    "            if words_limit == num:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bc001f37-c1b9-499b-8128-127892fda6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовим таблицу слов\n",
    "def create_words_txt(fname='exp/words.txt', dic_fname='timit/TIMITDIC.TXT'):\n",
    "    fname = Path(fname)\n",
    "    fname.parent.mkdir(exist_ok=True, parents=True)\n",
    "    words = ['<eps>'] + [w for w, _ in sorted(load_lexicon_file(dic_fname))] + ['<s>', '</s>']\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(''.join(f'{w} {i}\\n' for i, w in enumerate(words)))\n",
    "create_words_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b2c09-14f1-4c5a-aa8c-c7c0efb6d9fe",
   "metadata": {},
   "source": [
    "# T transducer\n",
    "\n",
    "Создадим первый wfst. \n",
    "\n",
    "В гибридном пайплайне системы распознавания речи задача первых двух wfst (H и C) - это перевести предсказанные с помощью акустической модели классы в последовательность фонем. В нашем случае все значительно легче, так как АМ (из работы №3) уже учится предсказывать фонемы, поэтому мы не будем использовать HС трансдьюсеры. \n",
    "\n",
    "В этой работе будет использоваться простой трасдьюсер T. Этот wfst нужен для преобразования вероятностей фонем, которые оценивает AM, в их правдоподобие. Используя теорему Баеса, сделать это преобразование легко. Надо просто добавить к скорам фонем их априорную вероятность.\n",
    "\n",
    "\n",
    "T.wfst состоит из одного состояния и N петель, где N - это количество фонем. Каждая петля соответствует определенной фонеме и хранит логарифм ее априорной вероятности.\n",
    "\n",
    "![image](resources/lab4/H_example.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "572a9a8b-8973-4453-91d9-203a18dd7993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.0 (20241103.1931)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"4735pt\" height=\"189pt\"\n",
       " viewBox=\"0.00 0.00 4734.50 188.59\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184.59)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-184.59 4730.5,-184.59 4730.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90.33\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.24,-91.83C64.02,-91.86 72,-91.36 72,-90.33 72,-89.75 69.48,-89.34 65.53,-89.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.83,-85.6 55.75,-88.87 65.67,-92.6 65.83,-85.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pau:pau:&#45;2.07</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.04,-92.56C90.73,-94.09 147,-93.35 147,-90.33 147,-87.61 101.18,-86.74 65.28,-87.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.42,-84.21 55.55,-88.05 65.66,-91.21 65.42,-84.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pau:pau:&#45;4.39</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.82,-93.01C109.49,-96.13 222,-95.24 222,-90.33 222,-85.74 123.52,-84.66 65.24,-87.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.15,-83.6 55.33,-87.58 65.49,-90.59 65.15,-83.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"252\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sh:sh:&#45;4.00</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.79,-93.43C122.49,-98.14 282,-97.11 282,-90.33 282,-83.89 138.28,-82.64 64.92,-86.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.07,-83.04 55.3,-87.14 65.49,-90.03 65.07,-83.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"310.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ix:ix:&#45;3.46</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54,-93.81C134.16,-100.15 339,-98.99 339,-90.33 339,-82.05 151.77,-80.63 65.27,-86.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.24,-82.55 55.51,-86.75 65.74,-89.54 65.24,-82.55\"/>\n",
       "<text text-anchor=\"middle\" x=\"370.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hv:hv:&#45;5.19</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.88,-94.08C145.38,-102.16 402,-100.91 402,-90.33 402,-80.16 165.04,-78.61 65.33,-85.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.08,-82.18 55.38,-86.46 65.63,-89.16 65.08,-82.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"432.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eh:eh:&#45;3.70</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.79,-94.33C155.7,-104.17 463.5,-102.84 463.5,-90.33 463.5,-78.25 176.38,-76.59 64.94,-85.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.95,-81.85 55.3,-86.2 65.56,-88.82 64.95,-81.85\"/>\n",
       "<text text-anchor=\"middle\" x=\"498\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dcl:dcl:&#45;4.03</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.75,-94.52C166.53,-106.18 532.5,-104.78 532.5,-90.33 532.5,-76.33 189.05,-74.58 65.05,-85.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.89,-81.58 55.26,-86 65.54,-88.55 64.89,-81.58\"/>\n",
       "<text text-anchor=\"middle\" x=\"561\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">jh:jh:&#45;5.30</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.48,-94.7C174.83,-108.19 589.5,-106.73 589.5,-90.33 589.5,-74.41 198.77,-72.57 64.88,-84.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.59,-81.32 54.99,-85.81 65.29,-88.29 64.59,-81.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"618\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ih:ih:&#45;3.57</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.55,-94.9C183.6,-110.19 646.5,-108.67 646.5,-90.33 646.5,-72.49 208.57,-70.56 64.94,-84.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.62,-81.06 55.05,-85.6 65.37,-88.02 64.62,-81.06\"/>\n",
       "<text text-anchor=\"middle\" x=\"671.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d:d:&#45;5.16</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.55,-95.08C191.19,-112.2 696,-110.61 696,-90.33 696,-70.56 216.5,-68.56 64.73,-84.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.6,-80.8 55.05,-85.41 65.39,-87.76 64.6,-80.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"726.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ah:ah:&#45;4.24</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.47,-95.22C199.66,-114.2 757.5,-112.57 757.5,-90.33 757.5,-68.63 226.56,-66.55 64.9,-84.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.49,-80.61 54.97,-85.27 65.31,-87.56 64.49,-80.61\"/>\n",
       "<text text-anchor=\"middle\" x=\"792\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">kcl:kcl:&#45;3.70</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.18,-95.3C208.08,-116.21 826.5,-114.55 826.5,-90.33 826.5,-66.66 236.14,-64.54 64.57,-83.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.18,-80.48 54.69,-85.18 65.04,-87.43 64.18,-80.48\"/>\n",
       "<text text-anchor=\"middle\" x=\"851.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">k:k:&#45;4.04</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.37,-95.47C215.69,-118.21 876,-116.5 876,-90.33 876,-64.74 244.39,-62.54 64.78,-83.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.35,-80.26 54.87,-85 65.24,-87.2 64.35,-80.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"899.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">s:s:&#45;2.82</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.11,-95.57C221.72,-120.21 922.5,-118.47 922.5,-90.33 922.5,-62.78 250.84,-60.53 64.42,-83.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.05,-80.1 54.61,-84.89 64.98,-87.03 64.05,-80.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"954\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ux:ux:&#45;4.23</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.12,-95.67C229.87,-122.22 985.5,-120.44 985.5,-90.33 985.5,-60.82 259.83,-58.52 64.42,-83.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.05,-79.95 54.62,-84.79 65,-86.89 64.05,-79.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"1010.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">q:q:&#45;4.11</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.12,-95.78C236.74,-124.22 1035,-122.4 1035,-90.33 1035,-58.87 266.85,-56.52 64.27,-83.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.02,-79.79 54.62,-84.67 65.01,-86.72 64.02,-79.79\"/>\n",
       "<text text-anchor=\"middle\" x=\"1065.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">en:en:&#45;5.51</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.1,-95.87C244.45,-126.22 1096.5,-124.38 1096.5,-90.33 1096.5,-56.91 275.78,-54.52 64.43,-83.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.99,-79.66 54.6,-84.57 65,-86.59 63.99,-79.66\"/>\n",
       "<text text-anchor=\"middle\" x=\"1131\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gcl:gcl:&#45;4.85</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.01,-95.93C252.4,-128.23 1165.5,-126.36 1165.5,-90.33 1165.5,-54.93 284.22,-52.51 64.17,-83.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.88,-79.57 54.5,-84.51 64.91,-86.49 63.88,-79.57\"/>\n",
       "<text text-anchor=\"middle\" x=\"1190.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">g:g:&#45;5.46</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.95,-96.01C258.86,-130.23 1215,-128.34 1215,-90.33 1215,-52.97 291.26,-50.5 64.16,-82.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.8,-79.44 54.45,-84.42 64.86,-86.36 63.8,-79.44\"/>\n",
       "<text text-anchor=\"middle\" x=\"1237.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">r:r:&#45;3.58</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.1,-96.13C265.59,-132.23 1260,-130.3 1260,-90.33 1260,-51.02 298.33,-48.5 64.32,-82.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.92,-79.29 54.59,-84.3 65.01,-86.2 63.92,-79.29\"/>\n",
       "<text text-anchor=\"middle\" x=\"1287.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w:w:&#45;4.22</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.9,-96.17C271.99,-134.24 1315.5,-132.29 1315.5,-90.33 1315.5,-49.05 305.35,-46.5 64.13,-82.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.71,-79.2 54.4,-84.25 64.83,-86.11 63.71,-79.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"1346.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ao:ao:&#45;3.66</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53,-96.25C279.65,-136.24 1377,-134.26 1377,-90.33 1377,-47.08 313.68,-44.5 64.25,-82.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.79,-79.09 54.49,-84.16 64.93,-86 63.79,-79.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"1408.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dx:dx:&#45;5.20</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.67,-96.25C285.95,-138.24 1440,-136.27 1440,-90.33 1440,-45.09 320.63,-42.49 63.89,-82.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.44,-79.06 54.16,-84.16 64.6,-85.96 63.44,-79.06\"/>\n",
       "<text text-anchor=\"middle\" x=\"1475.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">axr:axr:&#45;3.93</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.64,-96.3C293.66,-140.24 1510.5,-138.26 1510.5,-90.33 1510.5,-43.11 329.05,-40.48 63.86,-82.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.41,-78.99 54.13,-84.11 64.58,-85.89 63.41,-78.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"1532.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">l:l:&#45;3.70</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.79,-96.4C300.16,-142.24 1554,-140.22 1554,-90.33 1554,-41.14 335.42,-38.48 63.9,-82.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.54,-78.86 54.28,-84.01 64.73,-85.75 63.54,-78.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"1578.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">y:y:&#45;4.83</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.92,-96.48C306.98,-144.25 1603.5,-142.19 1603.5,-90.33 1603.5,-39.18 342.19,-36.48 63.92,-82.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.65,-78.74 54.41,-83.91 64.86,-85.63 63.65,-78.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"1635\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">uh:uh:&#45;5.85</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.67,-96.49C313.28,-146.25 1666.5,-144.2 1666.5,-90.33 1666.5,-37.2 350.03,-34.48 63.96,-82.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.39,-78.71 54.16,-83.91 64.61,-85.6 63.39,-78.71\"/>\n",
       "<text text-anchor=\"middle\" x=\"1691.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">n:n:&#45;3.61</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.69,-96.55C319.7,-148.25 1716,-146.18 1716,-90.33 1716,-35.22 356.27,-32.47 63.84,-82.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.4,-78.62 54.18,-83.84 64.65,-85.51 63.4,-78.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"1746\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ae:ae:&#45;3.17</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.72,-96.6C326.7,-150.25 1776,-148.16 1776,-90.33 1776,-33.23 363.26,-30.47 63.72,-82.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.41,-78.54 54.21,-83.79 64.67,-85.43 63.41,-78.54\"/>\n",
       "<text text-anchor=\"middle\" x=\"1805.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">m:m:&#45;4.08</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.69,-96.64C333.39,-152.25 1834.5,-150.15 1834.5,-90.33 1834.5,-31.27 371.26,-28.47 63.97,-81.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.37,-78.48 54.18,-83.75 64.65,-85.36 63.37,-78.48\"/>\n",
       "<text text-anchor=\"middle\" x=\"1866\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oy:oy:&#45;4.86</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.63,-96.67C340.18,-154.26 1897.5,-152.14 1897.5,-90.33 1897.5,-29.27 377.96,-26.47 63.72,-81.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.3,-78.43 54.12,-83.71 64.59,-85.31 63.3,-78.43\"/>\n",
       "<text text-anchor=\"middle\" x=\"1928.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ax:ax:&#45;4.40</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.52,-96.69C346.65,-156.26 1959,-154.14 1959,-90.33 1959,-27.27 384.22,-24.46 63.4,-81.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.18,-78.38 54.01,-83.69 64.49,-85.26 63.18,-78.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"1990.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dh:dh:&#45;4.89</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.37,-96.69C353,-158.26 2022,-156.14 2022,-90.33 2022,-25.29 391.89,-22.46 63.5,-81.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.02,-78.36 53.86,-83.68 64.34,-85.23 63.02,-78.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"2053.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">tcl:tcl:&#45;3.66</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.64,-96.78C360.88,-160.26 2085,-158.11 2085,-90.33 2085,-23.31 399.39,-20.46 63.55,-81.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.28,-78.25 54.12,-83.59 64.61,-85.12 63.28,-78.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"2113.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">iy:iy:&#45;3.11</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.39,-96.77C366.49,-162.26 2142,-160.12 2142,-90.33 2142,-21.33 406.15,-18.46 63.52,-81.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.01,-78.24 53.87,-83.6 64.36,-85.11 63.01,-78.24\"/>\n",
       "<text text-anchor=\"middle\" x=\"2166.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v:v:&#45;4.77</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.56,-96.85C373.3,-164.26 2191.5,-162.09 2191.5,-90.33 2191.5,-19.36 413.03,-16.45 63.67,-81.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.18,-78.15 54.04,-83.52 64.53,-85.01 63.18,-78.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"2214\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f:f:&#45;4.13</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.21,-96.81C377.88,-166.27 2236.5,-164.1 2236.5,-90.33 2236.5,-17.37 418.5,-14.45 63.49,-81.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.81,-78.15 53.69,-83.55 64.18,-85.01 62.81,-78.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"2258.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">t:t:&#45;4.21</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.31,-96.88C384.14,-168.27 2280,-166.08 2280,-90.33 2280,-15.39 424.65,-12.45 63.54,-81.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.9,-78.07 53.79,-83.48 64.29,-84.93 62.9,-78.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2314.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pcl:pcl:&#45;4.34</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.46,-96.93C391.75,-170.27 2349,-168.06 2349,-90.33 2349,-13.41 432.63,-10.45 63.67,-81.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.04,-78 53.94,-83.42 64.44,-84.86 63.04,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"2383.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ow:ow:&#45;3.96</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.58,-96.99C399.24,-172.27 2418,-170.05 2418,-90.33 2418,-11.43 440.43,-8.45 63.76,-81.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.15,-77.94 54.06,-83.37 64.56,-84.79 63.15,-77.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"2449.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hh:hh:&#45;5.43</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.39,-96.97C405.25,-174.27 2481,-172.05 2481,-90.33 2481,-9.42 446.6,-6.44 63.5,-81.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.96,-77.93 53.88,-83.38 64.38,-84.78 62.96,-77.93\"/>\n",
       "<text text-anchor=\"middle\" x=\"2511.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ch:ch:&#45;5.30</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.45,-97.01C412.08,-176.27 2542.5,-174.04 2542.5,-90.33 2542.5,-7.44 453.49,-4.44 63.48,-81.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63,-77.87 53.93,-83.34 64.43,-84.73 63,-77.87\"/>\n",
       "<text text-anchor=\"middle\" x=\"2577\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">bcl:bcl:&#45;4.77</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.49,-97.05C419.19,-178.27 2611.5,-176.03 2611.5,-90.33 2611.5,-5.47 461.8,-2.44 63.75,-81.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.04,-77.83 53.97,-83.3 64.48,-84.68 63.04,-77.83\"/>\n",
       "<text text-anchor=\"middle\" x=\"2636.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b:b:&#45;5.92</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.47,-97.07C425.19,-180.27 2661,-178.02 2661,-90.33 2661,-3.47 467.57,-0.44 63.62,-81.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.02,-77.78 53.96,-83.27 64.46,-84.63 63.02,-77.78\"/>\n",
       "<text text-anchor=\"middle\" x=\"2691\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">aa:aa:&#45;3.63</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.46,-97.1C431.63,-182.27 2721,-180.02 2721,-90.33 2721,-1.48 473.91,1.56 63.5,-81.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.99,-77.75 53.94,-83.25 64.45,-84.59 62.99,-77.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"2756.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">em:em:&#45;7.25</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.44,-97.11C438.48,-184.27 2791.5,-182.01 2791.5,-90.33 2791.5,0.5 481.94,3.56 63.69,-81.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.97,-77.72 53.92,-83.23 64.43,-84.57 62.97,-77.72\"/>\n",
       "<text text-anchor=\"middle\" x=\"2823\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ng:ng:&#45;5.15</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.38,-97.12C444.85,-186.27 2854.5,-184.01 2854.5,-90.33 2854.5,2.5 488.18,5.57 63.51,-81.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.9,-77.69 53.86,-83.22 64.37,-84.54 62.9,-77.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"2885.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ay:ay:&#45;3.72</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-97.13C451.03,-188.28 2916,-186.01 2916,-90.33 2916,4.51 494.17,7.57 63.29,-81.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.81,-77.67 53.78,-83.21 64.3,-84.51 62.81,-77.67\"/>\n",
       "<text text-anchor=\"middle\" x=\"2944.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">th:th:&#45;5.33</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.5,-97.2C458.16,-190.28 2973,-187.99 2973,-90.33 2973,6.47 502.17,9.57 63.68,-81.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.01,-77.6 53.98,-83.14 64.49,-84.44 63.01,-77.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"3015\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ax&#45;h:ax&#45;h:&#45;7.04</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.42,-97.19C465.18,-192.28 3057,-189.99 3057,-90.33 3057,8.48 509.29,11.57 63.47,-81.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.92,-77.59 53.9,-83.15 64.41,-84.43 62.92,-77.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"3087.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ey:ey:&#45;3.89</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.28,-97.18C471.06,-194.28 3118.5,-191.99 3118.5,-90.33 3118.5,10.47 516.11,13.57 63.5,-81.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.77,-77.59 53.76,-83.15 64.27,-84.43 62.77,-77.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"3143.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">p:p:&#45;4.82</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.42,-97.24C477.65,-196.28 3168,-193.98 3168,-90.33 3168,12.45 522.13,15.57 63.47,-80.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.9,-77.52 53.9,-83.1 64.42,-84.35 62.9,-77.52\"/>\n",
       "<text text-anchor=\"middle\" x=\"3201.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">aw:aw:&#45;4.79</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.25,-97.22C483.6,-198.28 3235.5,-195.98 3235.5,-90.33 3235.5,14.45 529.1,17.57 63.47,-80.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.73,-77.53 53.73,-83.11 64.25,-84.36 62.73,-77.53\"/>\n",
       "<text text-anchor=\"middle\" x=\"3264\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">er:er:&#45;4.08</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.38,-97.27C490.4,-200.28 3292.5,-197.97 3292.5,-90.33 3292.5,16.44 535.37,19.57 63.42,-80.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.85,-77.46 53.86,-83.06 64.38,-84.3 62.85,-77.46\"/>\n",
       "<text text-anchor=\"middle\" x=\"3324\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nx:nx:&#45;6.23</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.17,-97.24C495.97,-202.28 3355.5,-199.98 3355.5,-90.33 3355.5,18.43 541.86,21.58 63.36,-80.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.63,-77.48 53.65,-83.09 64.17,-84.31 62.63,-77.48\"/>\n",
       "<text text-anchor=\"middle\" x=\"3379.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">z:z:&#45;3.80</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.26,-97.28C502.27,-204.28 3403.5,-201.96 3403.5,-90.33 3403.5,20.43 547.42,23.58 63.25,-80.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.71,-77.43 53.73,-83.05 64.26,-84.25 62.71,-77.43\"/>\n",
       "<text text-anchor=\"middle\" x=\"3431.25\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">el:el:&#45;5.10</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.34,-97.32C508.83,-206.28 3459,-203.95 3459,-90.33 3459,22.4 554.75,25.58 63.5,-80.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.8,-77.38 53.82,-83 64.35,-84.2 62.8,-77.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"3493.5\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">uw:uw:&#45;5.48</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.44,-97.36C515.89,-208.28 3528,-205.94 3528,-90.33 3528,24.4 561.31,27.58 63.39,-80.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.88,-77.33 53.91,-82.97 64.44,-84.15 62.88,-77.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"3558.75\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">zh:zh:&#45;7.05</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.16,-97.31C521.06,-210.28 3589.5,-207.96 3589.5,-90.33 3589.5,26.41 567.33,29.58 63.25,-80.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6,-77.37 53.63,-83.01 64.16,-84.19 62.6,-77.37\"/>\n",
       "<text text-anchor=\"middle\" x=\"3627\" y=\"-85.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eng:eng:&#45;8.42</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1dd1f3a6650>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_t_wfst(prior_file='resources/lab4/phone.priors', phones_fname='exp/phones.txt'):\n",
    "    \"\"\"Create T.wfst. just transduce AM probability to AM likelihood (Bayes theorem)\n",
    "    \"\"\"\n",
    "    symbols = SymbolsMap.from_file(phones_fname)\n",
    "    t_fst = WFST(isymbols=symbols, osymbols=symbols)\n",
    "    s = t_fst.get_start()\n",
    "    t_fst.set_final(s)\n",
    "    with open(prior_file) as f:\n",
    "        for ph, prior in map(str.split, f.readlines()):\n",
    "            log_prior = np.log(float(prior))\n",
    "            ph_id = symbols.get_id(ph)\n",
    "            t_fst.add_arc(s, Arc(ph_id, ph_id, log_prior, s))\n",
    "    return t_fst\n",
    "create_t_wfst().to_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5765ec-0a60-4dc1-9bdd-28975b45f971",
   "metadata": {},
   "source": [
    "# L transducer\n",
    "\n",
    "Lwfst - это WFST, который преобразует входные последовательности фонем в слова.\n",
    "\n",
    "Например, `pau pau ae1 ae1 ae1 ae1 ae1 ae1 r pau pau pau y y y y eh1 s s s pau` -> `air yes`\n",
    "\n",
    "Чтобы контролировать количество слов в выходной гипотезе, в L добавляется два веса: \n",
    "* word_insertion_penalty - штраф за добавление слова \n",
    "* stay_in_silence_penalty - штраф за пропуск паузы (фонемы pau)\n",
    "\n",
    "Пример Lwfst, построенного для слов `'em` `-knacks`, с параметрами word_insertion_penalty=0.5, stay_in_silence_penalty=0.01\n",
    "\n",
    "![image](./resources/lab4/L_example.svg)\n",
    "\n",
    "Фонема паузы pau обрабатывается особым способом: L.fst может пропускать ее, не генерируя ничего на выходе. Также у окончания каждой транскрипции слова есть две отдельные дуги, возвращающие FST в стартовое состояние. Это нужно для того, чтобы была возможность как обработать подряд идущие дупликаты последней фонемы на конце слова, так и перейти в стартовое состояние всего по одной финальной фонеме. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2249beae-7404-4634-9e7a-7f6a4a6bc05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.0 (20241103.1931)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"438pt\" height=\"310pt\"\n",
       " viewBox=\"0.00 0.00 438.47 309.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 305.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-305.5 434.47,-305.5 434.47,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"162\" cy=\"-283.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"162\" y=\"-278.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.53,-291.67C197.51,-292.45 207,-289.72 207,-283.5 207,-279.61 203.29,-277.09 197.83,-275.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.22,-272.45 188.04,-275.41 197.84,-279.44 198.22,-272.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"249.75\" y=\"-278.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pau:&lt;eps&gt;:0.01</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-195\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.16,-279.82C97.88,-275.48 33.38,-265.52 19.25,-247.5 13.96,-240.75 13.65,-231.93 15.36,-223.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"18.71,-224.51 18.23,-213.93 12.01,-222.5 18.71,-224.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.62\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ax:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200\" cy=\"-195\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.6,-266.53C176.2,-260.69 180.05,-253.94 183,-247.5 186.43,-240.02 189.49,-231.65 192.05,-223.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.33,-225.09 194.96,-214.5 188.65,-223.02 195.33,-225.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"225.51\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">n:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49.58,-205.29C63.96,-211.65 82.7,-220.76 98,-231 107.22,-237.17 108.19,-240.43 116.75,-247.5 122.82,-252.51 129.47,-257.78 135.76,-262.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"133.35,-265.21 143.41,-268.53 137.61,-259.66 133.35,-265.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"147.88\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">m:&#39;em:0.50</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.53,-203.17C62.51,-203.95 72,-201.22 72,-195 72,-191.11 68.29,-188.59 62.83,-187.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.22,-183.95 53.04,-186.91 62.84,-190.94 63.22,-183.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"111.38\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ax:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.53,-203.17C235.51,-203.95 245,-201.22 245,-195 245,-191.11 241.29,-188.59 235.83,-187.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"236.22,-183.95 226.04,-186.91 235.84,-190.94 236.22,-183.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.38\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">n:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"197\" cy=\"-106.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M199.41,-176.91C199,-165.26 198.46,-149.55 197.99,-136.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"201.5,-136.24 197.65,-126.36 194.5,-136.48 201.5,-136.24\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.77\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ae:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;3 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>3&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M221.53,-114.67C232.51,-115.45 242,-112.72 242,-106.5 242,-102.61 238.29,-100.09 232.83,-98.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.22,-95.45 223.04,-98.41 232.84,-102.44 233.22,-95.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"281\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ae:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"273\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"273\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">4</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.24,-90.43C221.9,-77.16 239.07,-57.62 252.5,-42.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.94,-44.85 258.91,-35.03 249.68,-40.23 254.94,-44.85\"/>\n",
       "<text text-anchor=\"middle\" x=\"278.18\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">k:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;0 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>4&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M293.32,-30.34C302.35,-36.31 312.42,-44.4 319,-54 359.02,-112.35 363.34,-152.29 327,-213 300.77,-256.82 240.26,-273.02 200.26,-279\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"200.02,-275.5 190.57,-280.29 200.95,-282.44 200.02,-275.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"391.1\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">s:&#45;knacks:0.50</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;4 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>4&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M297.53,-26.17C308.51,-26.95 318,-24.22 318,-18 318,-14.11 314.29,-11.59 308.83,-10.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.22,-6.95 299.04,-9.91 308.84,-13.94 309.22,-6.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.38\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">k:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1dd21399990>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_Lwfst_from_file(dic_fname='timit/TIMITDIC.TXT', \n",
    "                           phones_fname='exp/phones.txt', \n",
    "                           words_fname='exp/words.txt', \n",
    "                           words_limit=-1,\n",
    "                           word_insertion_penalty=0.5, \n",
    "                           stay_in_silence_penalty=0.01):\n",
    "    isymbols = SymbolsMap.from_file(phones_fname)\n",
    "    osymbols = SymbolsMap.from_file(words_fname)\n",
    "    sil_id = isymbols.get_id('pau')\n",
    "    l_wfst = WFST(isymbols=isymbols, osymbols=osymbols)\n",
    "    start = l_wfst.get_start()\n",
    "    ### Пропуск скольких угодно фонем тишины\n",
    "    l_wfst.add_arc(start, Arc(sil_id, 0, stay_in_silence_penalty, start))\n",
    "    # Стартовое состояние также является и финальным\n",
    "    l_wfst.set_final(start)\n",
    "    \n",
    "    for word, trans in load_lexicon_file(dic_fname, words_limit=words_limit):\n",
    "        tran_ids = [isymbols.get_id(t) for t in trans]\n",
    "        word_id = osymbols.get_id(word)\n",
    "        # TODO \n",
    "        # Добавьте последовательные дуги в wfst, определяющие транскрипцию слова\n",
    "        # Все дуги, кроме последней, на выходе имеют <eps>\n",
    "        # Последняя дуга выводит word_id и возвращает конечный автомат в стартовое (оно же финальное) состояние\n",
    "        # Каждая фонема может тянуться от 1 до inf кадров, поэтому помимо переходов в новое состояние, должны быть еще петли \n",
    "        \n",
    "        # Begin in the start state\n",
    "        current_state = start\n",
    "\n",
    "        # Create sequential arcs for each phoneme in the transcription\n",
    "        for i, phoneme_id in enumerate(tran_ids):\n",
    "            # Create a new state for the current phoneme if it's not the last phoneme\n",
    "            if i < len(tran_ids) - 1:\n",
    "                next_state = l_wfst.new_state()\n",
    "                # Add an arc to the next state with <eps> as the output label\n",
    "                l_wfst.add_arc(current_state, Arc(phoneme_id, 0, 0, next_state))\n",
    "                # Add self-loop for phoneme repetition\n",
    "                l_wfst.add_arc(next_state, Arc(phoneme_id, 0, 0, next_state))\n",
    "                # Move to the next state\n",
    "                current_state = next_state\n",
    "            else:\n",
    "                # Last phoneme arc: output the word_id and return to start\n",
    "                l_wfst.add_arc(current_state, Arc(phoneme_id, word_id, word_insertion_penalty, start))\n",
    "                \n",
    "        \n",
    "    return l_wfst\n",
    "create_Lwfst_from_file(words_limit=2, word_insertion_penalty=0.5, stay_in_silence_penalty=0.01).to_dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e518300-0337-4727-9312-c2027b73cb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c5fe16fdb04f35948d99dbd2f109e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Test failed for word \"absolute\" hyps=[]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m hyps[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m start, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest failed for word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyps\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m       \n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest 2.a passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtest_l_wfst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[114], line 25\u001b[0m, in \u001b[0;36mtest_l_wfst\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# choose only final hyp\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# ограничение веса нужно, чтобы отсечь гипотезы где комбинация других слов мапится на теже фонемы\u001b[39;00m\n\u001b[0;32m     23\u001b[0m hyps \u001b[38;5;241m=\u001b[39m [(l, w, s) \u001b[38;5;28;01mfor\u001b[39;00m l, w, s \u001b[38;5;129;01min\u001b[39;00m hyps \u001b[38;5;28;01mif\u001b[39;00m l_wfst\u001b[38;5;241m.\u001b[39mfinal_score(s) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m]\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hyps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trans) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m , \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest failed for word \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyps\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hyps[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eps>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;241m==\u001b[39m [word], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest failed for word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyps\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hyps[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest failed for word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyps\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Test failed for word \"absolute\" hyps=[]"
     ]
    }
   ],
   "source": [
    "def test_l_wfst():\n",
    "    l_wfst = create_Lwfst_from_file(dic_fname='timit/TIMITDIC.TXT', \n",
    "                                    word_insertion_penalty=1.0, stay_in_silence_penalty=0.01)\n",
    "    start = l_wfst.get_start()\n",
    "    hyps = l_wfst.transduce(start, 'pau')\n",
    "    loop_hyp = [(l, w, s) for l, w, s in hyps if s == start]\n",
    "    assert (len(loop_hyp) == 1 \n",
    "            and loop_hyp[0][0] == '<eps>' \n",
    "            and loop_hyp[0][1] == 0.01), f\"{hyps}\\n skip pause test failed\"\n",
    "    # смотрим, что поданная на вход транскрипция корректно переводится в слово\n",
    "    # количество подряд идущих одинаковых фонем постепенно увеличивается от одной до 6\n",
    "    for i, (word, trans) in enumerate(tqdm(load_lexicon_file('timit/TIMITDIC.TXT', words_limit=100), total=100)):\n",
    "        hyps = [('', 0, l_wfst.get_start()), ]\n",
    "        # Breadth First Search\n",
    "        for t in [t for t in trans for _ in range(i//20+1)]:\n",
    "            # t repeated many times\n",
    "            new_hyps = []\n",
    "            for prev_l, prev_w, prev_s in hyps: \n",
    "                new_hyps.extend([(f\"{prev_l} {l}\", prev_w + w, s) for l,w,s in l_wfst.transduce(prev_s, t)])\n",
    "            hyps = new_hyps\n",
    "        # choose only final hyp\n",
    "        # ограничение веса нужно, чтобы отсечь гипотезы где комбинация других слов мапится на теже фонемы\n",
    "        hyps = [(l, w, s) for l, w, s in hyps if l_wfst.final_score(s) != float('inf') and w == 1.0]\n",
    "       \n",
    "        assert len(hyps) == 1 or len(trans) == 1 , f\"Test failed for word \\\"{word}\\\" {hyps=}\"\n",
    "        assert hyps[0][0].replace('<eps>', ' ').split() == [word], f\"Test failed for word {word} {hyps=}\"\n",
    "        assert hyps[0][1] == 1.0, f\"Test failed for word {word} {hyps=}\"\n",
    "        assert hyps[0][2] == start, f\"Test failed for word {word} {hyps=}\"       \n",
    "    print(\"test 2.a passed\")\n",
    "test_l_wfst()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085d806-f619-4941-b339-6c7ee21b3803",
   "metadata": {},
   "source": [
    "# Поиск наилучшей гипотезы распознавания с помощью beam search\n",
    "\n",
    "Для того, чтобы найти результат распознавания с помощью акустической модели и графа распознавания, надо выполнить поиск наилучшей гипотезы в графе. Для поиска лучшего пути будем использовать [лучевой поиск (Beam Search)](https://ru.wikipedia.org/wiki/%D0%9B%D1%83%D1%87%D0%B5%D0%B2%D0%BE%D0%B9_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA). В основе этого алгоритма лежит обход графа в ширину, но на каждом шаге рассматриваются только наиболее \"хорошие\" гипотезы, а остальные уничтожаются. Такая эвристика позволяет значительно ускорить процесс декодирования, но не всегда находит наилучший путь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bf3bb-9301-423e-9464-0479a9435f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(order=True)\n",
    "class Hypothesis:\n",
    "    # накопленный штраф гипотезы\n",
    "    score: float\n",
    "    # до какого кадра из входной последовательности дошла гипотеза\n",
    "    time: int\n",
    "    # накопленная последовательность слов\n",
    "    words: List[str]\n",
    "    # состояние графа декодирования \n",
    "    state: Any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142b823-9612-46ee-a54b-152b37d815ff",
   "metadata": {},
   "source": [
    "### HypothesesKeeper\n",
    "Реализуем специальную коллекцию, которая хранит все гипотезы распознавания и применяет прунинг (удаление ненужных гипотез). \n",
    "Прунинг делится на два типа: \n",
    "#### State pruning \n",
    "Eсли мы можем дойти до состояния X в момент времени T несколькими способами, то для дальнейшей обработки достаточно только наилучшего пути до состояния X. Эта гипотеза гарантированно будет лучше всех других гипотез, проходящих через точку (X, T). \n",
    "#### Beam pruning\n",
    "Эвристика лучевого поиска. Отсекаем все гипотезы, которые хуже, чем лучшая текущая гипотеза плюс beam_size. Вероятность того, что гипотезы с сильно худшим весом вдруг станут наилучшими, крайне мала, поэтому такая эвристика работает достаточно хорошо. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2485a-834f-499c-a356-b17983fd85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypothesesKeeper:\n",
    "    def __init__(self, init_hyps: List[Hypothesis] = [], beam_size=10):\n",
    "        self.state2hyp = {h.state: h for h in init_hyps}\n",
    "        self.beam_size = beam_size\n",
    "        if len(self.state2hyp) == 0:\n",
    "            self.set_best(None, float('inf'))\n",
    "        else:\n",
    "            min_hyp = min(self.state2hyp.values())\n",
    "            self.set_best(min_hyp.state, min_hyp.score)\n",
    "\n",
    "    def set_best(self, state, score):\n",
    "        self.best_state = state\n",
    "        self.best_score = score\n",
    "        \n",
    "    def get_best_hyp(self) -> Hypothesis:\n",
    "        \"\"\"return the best hyp based on self.best_state\"\"\"   \n",
    "        # TODO \n",
    "        # верните лучшую гипотезу \n",
    "        if self.best_state is None:\n",
    "            return None\n",
    "        return self.state2hyp.get(self.best_state, None)\n",
    "    \n",
    "    def prune(self):\n",
    "        self.state2hyp = {s:h for s, h in self.state2hyp.items() if not self.is_prunned_by_beam(h.score)}\n",
    "        \n",
    "    def tolist(self) -> List[Hypothesis]:\n",
    "        \"\"\"Return all hypotheses. Apply beam pruning\"\"\"\n",
    "        # TODO \n",
    "        # верните все гипотезы, которые находятся в луче поиска \n",
    "        \n",
    "        # First, prune the hypotheses based on the beam size\n",
    "        self.prune()\n",
    "        # Return the remaining hypotheses in a list\n",
    "        return list(self.state2hyp.values())\n",
    "    \n",
    "    def is_prunned_by_beam(self, score: float):\n",
    "        \"\"\"Return true if score greater than beam\"\"\"\n",
    "        # TODO \n",
    "        # верните True, если значение score находится вне луча поиска \n",
    "        # Sort hypotheses based on their score in ascending order (lowest score first)\n",
    "        sorted_hyps = sorted(self.state2hyp.values(), key=lambda h: h.score)\n",
    "        \n",
    "        # Get the score of the worst hypothesis that should be kept in the beam (i.e., the score of the beam_size-th hypothesis)\n",
    "        if len(sorted_hyps) < self.beam_size:\n",
    "            # If we have less than beam_size hypotheses, we keep all\n",
    "            return False\n",
    "        beam_threshold_score = sorted_hyps[self.beam_size - 1].score\n",
    "        \n",
    "        # Return True if the score is greater than the beam threshold (meaning it should be pruned)\n",
    "        return score > beam_threshold_score\n",
    "        \n",
    "    def is_prunned_by_state(self, state, score):\n",
    "        \"\"\"Returns true if the keeper already has a hyp in the same state and the score of this hyp is lower\"\"\"\n",
    "        # TODO\n",
    "        # верните True, если state уже имеет гипотезу, со скором лучше чем score\n",
    "        if state in self.state2hyp:\n",
    "            # Compare scores, if the current score is worse (higher), we prune\n",
    "            if self.state2hyp[state].score < score:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def append(self, hyp: Hypothesis):\n",
    "        \"\"\"Append new hyp into collection\"\"\"\n",
    "        if self.is_prunned_by_beam(hyp.score) or self.is_prunned_by_state(hyp.state, hyp.score):\n",
    "            return \n",
    "        self.state2hyp[hyp.state] = hyp\n",
    "        if hyp.score < self.best_score or self.best_state is None:\n",
    "            self.set_best(hyp.state, hyp.score)\n",
    "\n",
    "    def extend(self, hyps):\n",
    "        if isinstance(hyps, HypothesesKeeper):\n",
    "            hyps = hyps.tolist()\n",
    "        for h in hyps:\n",
    "            self.append(h)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.state2hyp)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '[' + \",\\n\".join(map(str, self.tolist())) + ']'\n",
    "\n",
    "    def describe(self):\n",
    "        # TODO \n",
    "        # посчитайте средний и максимальный скор хранящихся гипотез\n",
    "             \n",
    "        # Get all the scores from the stored hypotheses\n",
    "        scores = [hyp.score for hyp in self.state2hyp.values()]\n",
    "        \n",
    "        if scores:\n",
    "            # Calculate mean score and max score\n",
    "            mean_score = sum(scores) / len(scores)\n",
    "            max_score = max(scores)\n",
    "        else:\n",
    "            mean_score = 0\n",
    "            max_score = 0\n",
    "\n",
    "        return (\n",
    "            f\"{len(self)} hyps. \"\n",
    "            f\"Best {self.get_best_hyp()}. \"\n",
    "            f\"Mean score {mean_score:.2f}. \"\n",
    "            f\"Max {max_score:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349961f4-c05b-41e5-a514-277b69bde94d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Append didn't prune the input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m hyp \u001b[38;5;241m==\u001b[39m Hypothesis(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m3\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyp\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest 2.b passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtest_hyp_keeper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36mtest_hyp_keeper\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hyps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hyps\u001b[38;5;241m.\u001b[39mbest_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hyps\u001b[38;5;241m.\u001b[39mbest_score \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst append doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work well\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m hyps\u001b[38;5;241m.\u001b[39mappend(Hypothesis(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hyps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hyps\u001b[38;5;241m.\u001b[39mbest_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hyps\u001b[38;5;241m.\u001b[39mbest_score \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAppend didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt prune the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m hyps\u001b[38;5;241m.\u001b[39mappend(Hypothesis(\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m0\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hyps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hyps\u001b[38;5;241m.\u001b[39mbest_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hyps\u001b[38;5;241m.\u001b[39mbest_score \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAppend doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work well\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Append didn't prune the input"
     ]
    }
   ],
   "source": [
    "def test_hyp_keeper():\n",
    "    hyps = HypothesesKeeper(beam_size=10)\n",
    "    hyps.append(Hypothesis(10, 0, ['a'],  1))\n",
    "    assert len(hyps) == 1 and hyps.best_state == 1 and hyps.best_score == 10, f\"First append doesn't work well\"\n",
    "    hyps.append(Hypothesis(100, 0, ['err'], 2))\n",
    "    assert len(hyps) == 1 and hyps.best_state == 1 and hyps.best_score == 10, f\"Append didn't prune the input\"\n",
    "    \n",
    "    hyps.append(Hypothesis(11, 0, ['b'], 2))\n",
    "    assert len(hyps) == 2 and hyps.best_state == 1 and hyps.best_score == 10, f\"Append doesn't work well\"\n",
    "    hyps.append(Hypothesis(12, 0, ['err2'], 2))\n",
    "    assert len(hyps) == 2 and hyps.state2hyp[2].score == 11, f\"Append didn't prune the input\"\n",
    "    \n",
    "    hyps.append(Hypothesis(0, 0, ['c'], 3))\n",
    "    assert hyps.best_state == 3 and hyps.best_score == 0, f\"Append didn't update best_* attributes\"\n",
    "\n",
    "    hyps_list = hyps.tolist()\n",
    "    assert len(hyps_list) == 2, f\"tolist didn't prune the output\"\n",
    "\n",
    "    hyp = hyps.get_best_hyp()\n",
    "    assert hyp == Hypothesis(0, 0, ['c'], 3), f\"{hyp=}\"\n",
    "    print(\"test 2.b passed!\")\n",
    "test_hyp_keeper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bbcf8-e7db-4670-8a93-f13760cdcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearcher:\n",
    "    def __init__(self, am2phone_mapping, graph: AbstractWFST, beam_size=3):\n",
    "        self.am2phone_mapping = am2phone_mapping\n",
    "        self.graph = graph\n",
    "        self.beam_size = beam_size\n",
    "        \n",
    "    def decode(self, phonemes_nll: np.ndarray):\n",
    "        \"\"\"\n",
    "        Decoding input phonemes negative loglikelihood into word level hypthesis\n",
    "        phonemes_logprobs.shape is (Time, num_phones)\n",
    "        \"\"\"\n",
    "        parent_hyps = HypothesesKeeper(init_hyps=[Hypothesis(0, -1, [], self.graph.get_start())], \n",
    "                                       beam_size=self.beam_size)\n",
    "        pbar = tqdm(phonemes_nll)\n",
    "        for new_time, frame_dist in enumerate(pbar):\n",
    "            new_hyps = HypothesesKeeper(beam_size=self.beam_size)\n",
    "            # print(parent_hyps)\n",
    "            for parent_hyp in parent_hyps.tolist():\n",
    "                assert parent_hyp.time + 1 == new_time, f\"Wrong time {new_time=}, \\n{parent_hyp=}\"\n",
    "                # TODO \n",
    "                # Продолжите гипотезу parent_hyp с помощью всех фонем и их вероятностей из frame_dist\n",
    "                # соханите новые гипотезы в new_hyps\n",
    "                # words гипотез не должен содержать <eps> \n",
    "                # score гипотезы равен сумме скора родителя, phone_nll и веса от transduce по графу\n",
    "                for i, phone_nll in enumerate(frame_dist):\n",
    "                    raise NotImplementedError()\n",
    "                # !!!!!!!!\n",
    "            new_hyps.prune()\n",
    "            parent_hyps = new_hyps\n",
    "            statictic_str = parent_hyps.describe()\n",
    "            pbar.set_description(statictic_str, refresh=False)\n",
    "        # TODO \n",
    "        # сформируйте список финальных гипотез\n",
    "        # пройдитесь по parent_hyps и добавьте ко всем гипотезам финальный вес \n",
    "        final_hyps = HypothesesKeeper(beam_size=self.beam_size)\n",
    "        \n",
    "        # !!!!!!!!!!!!!!!\n",
    "        for parent_hyp in parent_hyps.tolist():\n",
    "            final_score = self.graph.final_score(parent_hyp.state)\n",
    "            final_hyp = Hypothesis(parent_hyp.time + 1, \n",
    "                                parent_hyp.score + final_score,  # Add the final score to the hypothesis\n",
    "                                parent_hyp.word_sequence,\n",
    "                                parent_hyp.state)\n",
    "            final_hyps.add_hypothesis(final_hyp)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(f\"Found {len(final_hyps)} hypotheses\")\n",
    "        best_hyp = final_hyps.get_best_hyp()\n",
    "        return best_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecdf4e-0d13-4ea6-b8b2-1826c009ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_beam_search():\n",
    "    \n",
    "    graph = WFST(isymbols=SymbolsMap(id2symbol=['<eps>', 'pau', 'A', 'B']),\n",
    "                 osymbols=SymbolsMap(id2symbol=['<eps>', 'err', '_', 'a', 'b']))\n",
    "    s = graph.new_state()\n",
    "    graph.add_arc(0, Arc(2, 3, 2.0, s))\n",
    "    graph.add_arc(0, Arc(3, 4, 3.0, s))\n",
    "    graph.add_arc(s, Arc(1, 2, 1.0, 0))\n",
    "    graph.set_final(s)\n",
    "                 \n",
    "    searcher = BeamSearcher(am2phone_mapping={0: 'pau', 1: 'A', 2: 'B'}, graph=graph, beam_size=10)\n",
    "    #                   p  A  B\n",
    "    logits = np.array([[-1, -1, -1],\n",
    "                       [0, 0, 0],\n",
    "                       [-100, -100, -100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((-1+2) + (-0+1) + (-100+2) , 2, ['a', '_', 'a'], s), best_hyp\n",
    "    logits = np.array([[1, 4, 1],\n",
    "                       [100, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((1+3) + (100+1) + (100+2) , 2, ['b', '_', 'a'], s), best_hyp\n",
    "    \n",
    "    searcher.graph.add_arc(0, Arc(2, 0, 5.0, 0))\n",
    "    logits = np.array([[1, 2, 0],\n",
    "                       [10, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((2+5) + (0+5) + (100+2) , 2, ['a'], s), best_hyp\n",
    "\n",
    "    logits = np.array([[1, 20, 4], # beam pruning must remove A-loop hypothesis\n",
    "                       [1000, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((4+3) + (1000+1) + (100+2) , 2, ['b', '_', 'a'], s), best_hyp\n",
    "\n",
    "    searcher.graph.add_arc(0, Arc(3, 0, -1, 0))\n",
    "    logits = np.array([[1, 2, 0],\n",
    "                       [10, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((0-1) + (0-1) + (100+2) , 2, ['a'], s), best_hyp\n",
    "\n",
    "    print(f\"Test 2.c passed\")\n",
    "test_beam_search()                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c85696-b4fa-487a-845c-c8d0203a4753",
   "metadata": {},
   "source": [
    "### WFST представление нграмной языковой модели \n",
    "Для подсчета языковой модели будем использовать библиотеку kenlm. Данная библиотека позволяет подсчитывать языковую вероятность с помощью нграмной языковой модели. \n",
    "\n",
    "Создадим обертку над kenlm.Model, реализующую интерфейс AbstractWFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192ca61-a0c2-43f0-8954-83840989c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kenlmLikeWFST(AbstractWFST):\n",
    "    \"\"\"Оборачиваем kenlm в унифицированный интерфейс\"\"\"\n",
    "    def __init__(self, lm: kenlm.Model, lmwt_factor=0.01):\n",
    "        self.lm = lm\n",
    "        self.lmwt_factor = lmwt_factor\n",
    "        \n",
    "    def get_start(self):\n",
    "        state = kenlm.State()\n",
    "        self.lm.BeginSentenceWrite(state)\n",
    "        return state\n",
    "        \n",
    "    def final_score(self, state: kenlm.State):\n",
    "        logprob = self.lm.BaseScore(state, \"</s>\", kenlm.State())\n",
    "        return self._log10_to_nll(logprob)\n",
    "        \n",
    "    def _log10_to_nll(self, logprob):\n",
    "        \"\"\"переводим в negative натуральный логирифм \"\"\"\n",
    "        return - np.log(10**logprob)\n",
    "        \n",
    "    def transduce(self, state: kenlm.State, ilabel: str):\n",
    "        assert isinstance(ilabel, str), ilabel\n",
    "        if ilabel == '<eps>':\n",
    "            # skip <eps> input\n",
    "            return [('<eps>', 0, state), ]\n",
    "        state2 = kenlm.State()\n",
    "        logprob = self.lm.BaseScore(state, ilabel, state2)\n",
    "        return [(ilabel, self.lmwt_factor * self._log10_to_nll(logprob), state2), ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd124f1a-0232-48c0-933a-ef5247ab46ed",
   "metadata": {},
   "source": [
    "## Запускаем декодирование\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39324b-a225-4e8d-914a-5dd99e1ddb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заранее подготовленные negative log probability от модели из работы №3\n",
    "with ReadHelper('ark:resources/lab4/test_am_nlogprobs.ark') as am_nlogprob_reader:\n",
    "    am_nlogprobs = {uri: am_logprob  for uri, am_logprob in am_nlogprob_reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745f0de-a7e2-4b01-b363-cb984ea4d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She had your dark suit in greasy wash water all year\n"
     ]
    }
   ],
   "source": [
    "# Выбираем один пример для отладки пайплайна\n",
    "example = am_nlogprobs['timit/data/TEST/DR1/FAKS0/SA1']\n",
    "with open('timit/data/TEST/DR1/FAKS0/SA1.TXT') as f:\n",
    "    example_ref = ' '.join(f.read().replace('.', ' ').split()[2:])\n",
    "print(example_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3d464-41b5-46a4-95bc-d0e4900549cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fst = create_t_wfst()\n",
    "l_fst = create_Lwfst_from_file(stay_in_silence_penalty=0.0, word_insertion_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549d2da-0501-4be5-be8b-ec6ea8085f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пробуем разные варианты\n",
    "ASR = BeamSearcher(am2phone_mapping=AM_PHONES, \n",
    "                   graph=OnTheFlyCompose([t_fst, l_fst]),\n",
    "                   beam_size=0)   \n",
    "hyp = ASR.decode(example)\n",
    "print(hyp, \"WER: \", jiwer.wer(example_ref, ' '.join(hyp.words)))\n",
    "\n",
    "# Hypothesis(score=inf, time=197, words=[], state=(0, 28541)) WER:  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8588eab-859f-4b12-9030-e71e0be28573",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR = BeamSearcher(am2phone_mapping=AM_PHONES, \n",
    "                   graph=OnTheFlyCompose([t_fst, l_fst]),\n",
    "                   beam_size=6)\n",
    "hyp = ASR.decode(example)\n",
    "print(hyp, \"WER: \", jiwer.wer(example_ref, ' '.join(hyp.words)))\n",
    "\n",
    "# Hypothesis(score=-634.8072872315479,\n",
    "# time=197, \n",
    "# words=['she', 'had', 'your', 'dark', 'suit', 'in', 'greasy', 'wash', 'water', 'all', 'year'],\n",
    "# state=(0, 0)) WER:  0.09090909090909091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37279e-6610-44e7-b43f-ea831ef52d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = kenlmLikeWFST(kenlm.LanguageModel('resources/lab4/3gram.arpa'), lmwt_factor=0.2)\n",
    "ASR = BeamSearcher(am2phone_mapping=AM_PHONES, \n",
    "                   graph=OnTheFlyCompose([t_fst, l_fst, lm]),\n",
    "                   beam_size=6)\n",
    "hyp = ASR.decode(example)\n",
    "print(hyp, \"WER: \", jiwer.wer(example_ref, ' '.join(hyp.words)))\n",
    "\n",
    "# Hypothesis(score=-624.2866933483066, \n",
    "# time=197, \n",
    "# words=['she', 'had', 'your', 'dark', 'suit', 'in', 'greasy', 'wash', 'water', 'all', 'year'], \n",
    "# state=(0, 0, <kenlm.State object at 0x7fa93c7c1070>)) WER:  0.09090909090909091"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81474c81-99af-497a-a1f1-e9b85e8c39d5",
   "metadata": {},
   "source": [
    "### финальное задание\n",
    "Постройте графики зависимости WER на example и времени работы декодирования от таких параметров как: \n",
    "* lmwt_factor\n",
    "* stay_in_silence_penalty\n",
    "* word_insertion_penalty\n",
    "* beam\n",
    "\n",
    "Выберите оптимальные по соотношению WER/time параметры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20e693-bcb3-4937-907a-9c1546795a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654550e1-7c49-4008-94f5-60ae6d3b334c",
   "metadata": {},
   "source": [
    "# Дополнительное задание (2 балла)\n",
    "Декодируйте всю коллекцию am_nlogprobs с подобраными ранее параметрами и посчитайте WER на этой выборке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a85ca6-a620-4828-abc7-721df929452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
